{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a1048d8",
   "metadata": {},
   "source": [
    "# AI Stock Picks - Model Training Notebook\n",
    "\n",
    "This notebook trains the ML ensemble models for stock screening.\n",
    "\n",
    "## What this notebook does:\n",
    "1. Fetches historical stock data using yfinance\n",
    "2. Calculates technical indicators and fundamental metrics\n",
    "3. Trains XGBoost, Random Forest, and LightGBM models\n",
    "4. Saves trained models as .pkl files\n",
    "5. Provides download links for deployment\n",
    "\n",
    "## Usage:\n",
    "1. Run all cells in order\n",
    "2. Download the generated model files\n",
    "3. Upload to your server or commit to repo\n",
    "\n",
    "**Expected Runtime:** 10-30 minutes depending on data size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dc62b3",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b074e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q yfinance pandas numpy scikit-learn xgboost lightgbm ta-lib-binary joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24524678",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b02cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7ea31b",
   "metadata": {},
   "source": [
    "## Step 3: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8875690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "CONFIG = {\n",
    "    'universe': ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA', 'META', 'TSLA', 'JPM', 'V', 'JNJ',\n",
    "                 'WMT', 'PG', 'MA', 'UNH', 'DIS', 'HD', 'BAC', 'ADBE', 'CRM', 'NFLX',\n",
    "                 'XOM', 'CVX', 'PFE', 'KO', 'PEP', 'COST', 'ABBV', 'MRK', 'TMO', 'AVGO',\n",
    "                 'LLY', 'ABT', 'DHR', 'NKE', 'ACN', 'TXN', 'NEE', 'ORCL', 'MCD', 'QCOM',\n",
    "                 'BMY', 'LIN', 'PM', 'UNP', 'AMD', 'HON', 'UPS', 'LOW', 'RTX', 'INTC'],\n",
    "    'period': '3y',  # 3 years of data\n",
    "    'forward_days': 30,  # Predict 30-day forward returns\n",
    "    'cv_splits': 5,  # Time series cross-validation splits\n",
    "    'min_history': 250,  # Minimum trading days required\n",
    "}\n",
    "\n",
    "print(f\"Training on {len(CONFIG['universe'])} stocks\")\n",
    "print(f\"Period: {CONFIG['period']}\")\n",
    "print(f\"Target: {CONFIG['forward_days']}-day forward returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6487f01",
   "metadata": {},
   "source": [
    "## Step 4: Fetch Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aae5777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_stock_data(symbols, period='3y'):\n",
    "    \"\"\"Fetch historical data for multiple stocks\"\"\"\n",
    "    data = {}\n",
    "    failed = []\n",
    "    \n",
    "    for i, symbol in enumerate(symbols, 1):\n",
    "        try:\n",
    "            print(f\"[{i}/{len(symbols)}] Fetching {symbol}...\", end=' ')\n",
    "            ticker = yf.Ticker(symbol)\n",
    "            hist = ticker.history(period=period)\n",
    "            \n",
    "            if len(hist) >= CONFIG['min_history']:\n",
    "                data[symbol] = hist\n",
    "                print(f\"✅ {len(hist)} days\")\n",
    "            else:\n",
    "                print(f\"❌ Insufficient data ({len(hist)} days)\")\n",
    "                failed.append(symbol)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {e}\")\n",
    "            failed.append(symbol)\n",
    "    \n",
    "    print(f\"\\n✅ Successfully loaded {len(data)} stocks\")\n",
    "    if failed:\n",
    "        print(f\"❌ Failed: {', '.join(failed)}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Fetch data\n",
    "stock_data = fetch_stock_data(CONFIG['universe'], CONFIG['period'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536850fd",
   "metadata": {},
   "source": [
    "## Step 5: Calculate Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c9b8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rsi(prices, period=14):\n",
    "    \"\"\"Calculate Relative Strength Index\"\"\"\n",
    "    delta = prices.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def calculate_macd(prices, fast=12, slow=26, signal=9):\n",
    "    \"\"\"Calculate MACD\"\"\"\n",
    "    ema_fast = prices.ewm(span=fast).mean()\n",
    "    ema_slow = prices.ewm(span=slow).mean()\n",
    "    macd = ema_fast - ema_slow\n",
    "    signal_line = macd.ewm(span=signal).mean()\n",
    "    return macd - signal_line\n",
    "\n",
    "def calculate_bollinger_bands(prices, period=20, std_dev=2):\n",
    "    \"\"\"Calculate Bollinger Bands position\"\"\"\n",
    "    sma = prices.rolling(window=period).mean()\n",
    "    std = prices.rolling(window=period).std()\n",
    "    upper = sma + (std * std_dev)\n",
    "    lower = sma - (std * std_dev)\n",
    "    return (prices - lower) / (upper - lower)  # Position within bands\n",
    "\n",
    "def calculate_features(df):\n",
    "    \"\"\"Calculate all technical features\"\"\"\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Price-based features\n",
    "    features['returns_1d'] = df['Close'].pct_change()\n",
    "    features['returns_5d'] = df['Close'].pct_change(5)\n",
    "    features['returns_20d'] = df['Close'].pct_change(20)\n",
    "    features['returns_60d'] = df['Close'].pct_change(60)\n",
    "    \n",
    "    # Moving averages\n",
    "    for period in [5, 10, 20, 50, 200]:\n",
    "        features[f'sma_{period}'] = df['Close'].rolling(period).mean() / df['Close'] - 1\n",
    "    \n",
    "    # Technical indicators\n",
    "    features['rsi_14'] = calculate_rsi(df['Close'], 14)\n",
    "    features['rsi_7'] = calculate_rsi(df['Close'], 7)\n",
    "    features['macd'] = calculate_macd(df['Close'])\n",
    "    features['bb_position'] = calculate_bollinger_bands(df['Close'])\n",
    "    \n",
    "    # Volume features\n",
    "    features['volume_ratio'] = df['Volume'] / df['Volume'].rolling(20).mean()\n",
    "    features['volume_trend'] = df['Volume'].pct_change(5)\n",
    "    \n",
    "    # Volatility\n",
    "    features['volatility_20d'] = df['Close'].pct_change().rolling(20).std()\n",
    "    features['volatility_60d'] = df['Close'].pct_change().rolling(60).std()\n",
    "    \n",
    "    # High-Low range\n",
    "    features['hl_ratio'] = (df['High'] - df['Low']) / df['Close']\n",
    "    features['close_position'] = (df['Close'] - df['Low']) / (df['High'] - df['Low'])\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"Calculating technical indicators for all stocks...\")\n",
    "features_dict = {}\n",
    "for symbol, df in stock_data.items():\n",
    "    features_dict[symbol] = calculate_features(df)\n",
    "    print(f\"✅ {symbol}: {features_dict[symbol].shape[1]} features\")\n",
    "\n",
    "print(f\"\\n✅ Features calculated for {len(features_dict)} stocks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8acf7a",
   "metadata": {},
   "source": [
    "## Step 6: Create Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb77862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(stock_data, features_dict, forward_days=30):\n",
    "    \"\"\"Create training dataset with forward returns as target\"\"\"\n",
    "    \n",
    "    all_features = []\n",
    "    all_targets = []\n",
    "    all_symbols = []\n",
    "    all_dates = []\n",
    "    \n",
    "    for symbol in stock_data.keys():\n",
    "        prices = stock_data[symbol]['Close']\n",
    "        features = features_dict[symbol]\n",
    "        \n",
    "        # Calculate forward returns (target)\n",
    "        forward_returns = prices.shift(-forward_days) / prices - 1\n",
    "        \n",
    "        # Align features and targets\n",
    "        valid_idx = features.notna().all(axis=1) & forward_returns.notna()\n",
    "        \n",
    "        if valid_idx.sum() > 0:\n",
    "            all_features.append(features[valid_idx])\n",
    "            all_targets.append(forward_returns[valid_idx])\n",
    "            all_symbols.extend([symbol] * valid_idx.sum())\n",
    "            all_dates.extend(features[valid_idx].index.tolist())\n",
    "    \n",
    "    # Combine all data\n",
    "    X = pd.concat(all_features, axis=0)\n",
    "    y = pd.concat(all_targets, axis=0)\n",
    "    \n",
    "    # Add metadata\n",
    "    meta = pd.DataFrame({\n",
    "        'symbol': all_symbols,\n",
    "        'date': all_dates\n",
    "    }, index=X.index)\n",
    "    \n",
    "    return X, y, meta\n",
    "\n",
    "print(\"Creating training dataset...\")\n",
    "X, y, meta = create_training_data(stock_data, features_dict, CONFIG['forward_days'])\n",
    "\n",
    "print(f\"\\n✅ Training data created:\")\n",
    "print(f\"   Samples: {len(X):,}\")\n",
    "print(f\"   Features: {X.shape[1]}\")\n",
    "print(f\"   Stocks: {meta['symbol'].nunique()}\")\n",
    "print(f\"   Date range: {meta['date'].min()} to {meta['date'].max()}\")\n",
    "print(f\"\\n📊 Target statistics:\")\n",
    "print(y.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f3a358",
   "metadata": {},
   "source": [
    "## Step 7: Feature Scaling & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ffbcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle any remaining NaN values\n",
    "X = X.fillna(X.median())\n",
    "y = y.fillna(0)\n",
    "\n",
    "# Scale features\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "\n",
    "# Store feature names\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "print(f\"✅ Features scaled using RobustScaler\")\n",
    "print(f\"✅ {len(feature_names)} feature names stored\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eba20e",
   "metadata": {},
   "source": [
    "## Step 8: Time Series Cross-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c2178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by date for time series split\n",
    "sort_idx = meta['date'].argsort()\n",
    "X_sorted = X_scaled.iloc[sort_idx]\n",
    "y_sorted = y.iloc[sort_idx]\n",
    "\n",
    "# Use 80% for training, 20% for final validation\n",
    "split_idx = int(len(X_sorted) * 0.8)\n",
    "X_train, X_test = X_sorted[:split_idx], X_sorted[split_idx:]\n",
    "y_train, y_test = y_sorted[:split_idx], y_sorted[split_idx:]\n",
    "\n",
    "print(f\"Training set: {len(X_train):,} samples\")\n",
    "print(f\"Test set: {len(X_test):,} samples\")\n",
    "print(f\"\\n✅ Time series split complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0101d662",
   "metadata": {},
   "source": [
    "## Step 9: Train XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b801abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training XGBoost model...\\n\")\n",
    "\n",
    "xgb_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 500,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'early_stopping_rounds': 50\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(**xgb_params)\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    verbose=50\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f\"\\n✅ XGBoost Training Complete\")\n",
    "print(f\"   MSE: {mse_xgb:.6f}\")\n",
    "print(f\"   MAE: {mae_xgb:.6f}\")\n",
    "print(f\"   R²: {r2_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16861b7",
   "metadata": {},
   "source": [
    "## Step 10: Train Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f47017",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Random Forest model...\\n\")\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': 300,\n",
    "    'max_depth': 12,\n",
    "    'min_samples_split': 10,\n",
    "    'min_samples_leaf': 5,\n",
    "    'max_features': 'sqrt',\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "rf_model = RandomForestRegressor(**rf_params)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"\\n✅ Random Forest Training Complete\")\n",
    "print(f\"   MSE: {mse_rf:.6f}\")\n",
    "print(f\"   MAE: {mae_rf:.6f}\")\n",
    "print(f\"   R²: {r2_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe877183",
   "metadata": {},
   "source": [
    "## Step 11: Train LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6f0789",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training LightGBM model...\\n\")\n",
    "\n",
    "lgb_params = {\n",
    "    'objective': 'regression',\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 500,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': 50\n",
    "}\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
    "lgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "y_pred_lgb = lgb_model.predict(X_test)\n",
    "mse_lgb = mean_squared_error(y_test, y_pred_lgb)\n",
    "mae_lgb = mean_absolute_error(y_test, y_pred_lgb)\n",
    "r2_lgb = r2_score(y_test, y_pred_lgb)\n",
    "\n",
    "print(f\"\\n✅ LightGBM Training Complete\")\n",
    "print(f\"   MSE: {mse_lgb:.6f}\")\n",
    "print(f\"   MAE: {mae_lgb:.6f}\")\n",
    "print(f\"   R²: {r2_lgb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ce3b7f",
   "metadata": {},
   "source": [
    "## Step 12: Evaluate Ensemble Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00afa776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble weights (can be tuned)\n",
    "WEIGHTS = {\n",
    "    'xgboost': 0.35,\n",
    "    'random_forest': 0.25,\n",
    "    'lightgbm': 0.40\n",
    "}\n",
    "\n",
    "# Weighted ensemble prediction\n",
    "y_pred_ensemble = (\n",
    "    WEIGHTS['xgboost'] * y_pred_xgb +\n",
    "    WEIGHTS['random_forest'] * y_pred_rf +\n",
    "    WEIGHTS['lightgbm'] * y_pred_lgb\n",
    ")\n",
    "\n",
    "# Evaluate ensemble\n",
    "mse_ensemble = mean_squared_error(y_test, y_pred_ensemble)\n",
    "mae_ensemble = mean_absolute_error(y_test, y_pred_ensemble)\n",
    "r2_ensemble = r2_score(y_test, y_pred_ensemble)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📊 FINAL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nXGBoost:       MSE={mse_xgb:.6f}, MAE={mae_xgb:.6f}, R²={r2_xgb:.4f}\")\n",
    "print(f\"Random Forest: MSE={mse_rf:.6f}, MAE={mae_rf:.6f}, R²={r2_rf:.4f}\")\n",
    "print(f\"LightGBM:      MSE={mse_lgb:.6f}, MAE={mae_lgb:.6f}, R²={r2_lgb:.4f}\")\n",
    "print(f\"\\n🎯 ENSEMBLE:    MSE={mse_ensemble:.6f}, MAE={mae_ensemble:.6f}, R²={r2_ensemble:.4f}\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c0f2e6",
   "metadata": {},
   "source": [
    "## Step 13: Save Models and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18176a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('trained_models', exist_ok=True)\n",
    "\n",
    "# Save models\n",
    "print(\"Saving models...\")\n",
    "joblib.dump(xgb_model, 'trained_models/xgboost.pkl')\n",
    "joblib.dump(rf_model, 'trained_models/random_forest.pkl')\n",
    "joblib.dump(lgb_model, 'trained_models/lightgbm.pkl')\n",
    "joblib.dump(scaler, 'trained_models/feature_engineer.pkl')\n",
    "print(\"✅ Models saved\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'trained_at': datetime.now().isoformat(),\n",
    "    'config': CONFIG,\n",
    "    'feature_names': feature_names,\n",
    "    'feature_count': len(feature_names),\n",
    "    'training_samples': len(X_train),\n",
    "    'test_samples': len(X_test),\n",
    "    'ensemble_weights': WEIGHTS,\n",
    "    'metrics': {\n",
    "        'xgboost': {'mse': float(mse_xgb), 'mae': float(mae_xgb), 'r2': float(r2_xgb)},\n",
    "        'random_forest': {'mse': float(mse_rf), 'mae': float(mae_rf), 'r2': float(r2_rf)},\n",
    "        'lightgbm': {'mse': float(mse_lgb), 'mae': float(mae_lgb), 'r2': float(r2_lgb)},\n",
    "        'ensemble': {'mse': float(mse_ensemble), 'mae': float(mae_ensemble), 'r2': float(r2_ensemble)}\n",
    "    },\n",
    "    'torch_available': False\n",
    "}\n",
    "\n",
    "with open('trained_models/metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(\"✅ Metadata saved\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ ALL MODELS SAVED SUCCESSFULLY\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"  📁 trained_models/xgboost.pkl\")\n",
    "print(\"  📁 trained_models/random_forest.pkl\")\n",
    "print(\"  📁 trained_models/lightgbm.pkl\")\n",
    "print(\"  📁 trained_models/feature_engineer.pkl\")\n",
    "print(\"  📁 trained_models/metadata.json\")\n",
    "print(\"\\n📥 Download these files and upload to your server's ml_models/ directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d80cf2",
   "metadata": {},
   "source": [
    "## Step 14: Test Prediction on Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2814667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prediction on a few samples\n",
    "sample_features = X_test.head(10)\n",
    "sample_actual = y_test.head(10)\n",
    "\n",
    "# Get predictions from all models\n",
    "pred_xgb = xgb_model.predict(sample_features)\n",
    "pred_rf = rf_model.predict(sample_features)\n",
    "pred_lgb = lgb_model.predict(sample_features)\n",
    "pred_ensemble = (\n",
    "    WEIGHTS['xgboost'] * pred_xgb +\n",
    "    WEIGHTS['random_forest'] * pred_rf +\n",
    "    WEIGHTS['lightgbm'] * pred_lgb\n",
    ")\n",
    "\n",
    "# Display results\n",
    "results = pd.DataFrame({\n",
    "    'Actual': sample_actual.values,\n",
    "    'Ensemble': pred_ensemble,\n",
    "    'XGBoost': pred_xgb,\n",
    "    'RandomForest': pred_rf,\n",
    "    'LightGBM': pred_lgb\n",
    "})\n",
    "\n",
    "print(\"\\n📊 Sample Predictions (30-day forward returns):\")\n",
    "print(results.round(4))\n",
    "print(\"\\n✅ Models are ready for deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78c55b8",
   "metadata": {},
   "source": [
    "## Step 15: Download Instructions\n",
    "\n",
    "### In Google Colab:\n",
    "1. Look for the `trained_models` folder in the file browser (left sidebar)\n",
    "2. Right-click on each file and select \"Download\"\n",
    "3. Download all 5 files:\n",
    "   - `xgboost.pkl`\n",
    "   - `random_forest.pkl`\n",
    "   - `lightgbm.pkl`\n",
    "   - `feature_engineer.pkl`\n",
    "   - `metadata.json`\n",
    "\n",
    "### Deployment:\n",
    "1. Upload these files to your server's `ml_models/` directory\n",
    "2. Or commit them to your GitHub repo (if < 100MB each)\n",
    "3. Update your environment variable: `ML_MODELS_ENABLED=true`\n",
    "4. Set `use_ml=True` in your pattern detector\n",
    "\n",
    "### Alternative - Download as ZIP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75754161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip file for easy download\n",
    "import shutil\n",
    "\n",
    "shutil.make_archive('trained_models', 'zip', 'trained_models')\n",
    "print(\"✅ Created trained_models.zip\")\n",
    "print(\"📥 Download this file from the Colab file browser\")\n",
    "\n",
    "# In Colab, you can also use:\n",
    "from google.colab import files\n",
    "files.download('trained_models.zip')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
