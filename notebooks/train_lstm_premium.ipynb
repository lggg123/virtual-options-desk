{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ca8a227",
   "metadata": {},
   "source": [
    "# Train LSTM Model for Premium Tier\n",
    "\n",
    "This notebook trains the Premium tier LSTM deep learning model (10% weight in ensemble)\n",
    "\n",
    "**Training Time:** 30-60 minutes  \n",
    "**GPU Required:** Yes (T4 or better recommended)  \n",
    "**Output:** lstm.pth, lstm_scaler.pkl, lstm_metadata.json  \n",
    "**Recommended:** Run in Google Colab with GPU runtime\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Quick Start Guide\n",
    "\n",
    "### For Google Colab Users (RECOMMENDED):\n",
    "\n",
    "1. **Upload this notebook to Colab**\n",
    "   - Go to [colab.research.google.com](https://colab.research.google.com)\n",
    "   - File ‚Üí Upload notebook ‚Üí Select this file\n",
    "\n",
    "2. **Enable GPU Runtime** ‚ö†Ô∏è IMPORTANT!\n",
    "   - Runtime ‚Üí Change runtime type\n",
    "   - Hardware accelerator ‚Üí **GPU** (select T4 or better)\n",
    "   - Click Save\n",
    "   - Without GPU, training will take HOURS instead of minutes!\n",
    "\n",
    "3. **Check GPU is enabled** (Cell 3 below)\n",
    "   - Run cell 3\n",
    "   - Should see: \"Using device: cuda\" ‚úÖ\n",
    "   - If you see \"cpu\", go back to step 2\n",
    "\n",
    "4. **Install packages** (Cell 2 below)\n",
    "   - Uncomment the install line\n",
    "   - Run the cell\n",
    "\n",
    "5. **Upload your stock list CSV** (Optional)\n",
    "   - If you have `eodhd_us_tickers.csv`, upload it:\n",
    "   ```python\n",
    "   from google.colab import files\n",
    "   uploaded = files.upload()\n",
    "   ```\n",
    "   - Or skip this - will download S&P 500 stocks automatically\n",
    "\n",
    "6. **Run all cells**\n",
    "   - Runtime ‚Üí Run all\n",
    "   - Go for a walk üö∂ (takes 30-60 minutes with GPU)\n",
    "\n",
    "7. **Download trained models**\n",
    "   - Run the last cell to download a zip file\n",
    "   - Upload `lstm.pth` and `lstm_scaler.pkl` to your `ml_models/` directory\n",
    "\n",
    "### For Local Users:\n",
    "\n",
    "**Prerequisites:**\n",
    "- NVIDIA GPU with CUDA support\n",
    "- PyTorch with CUDA installed\n",
    "\n",
    "1. **Install dependencies:**\n",
    "   ```bash\n",
    "   # Install PyTorch with CUDA (check pytorch.org for your system)\n",
    "   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "   \n",
    "   # Install other packages\n",
    "   pip install pandas numpy yfinance joblib scikit-learn\n",
    "   ```\n",
    "\n",
    "2. **Verify GPU:**\n",
    "   ```python\n",
    "   import torch\n",
    "   print(torch.cuda.is_available())  # Should print True\n",
    "   ```\n",
    "\n",
    "3. **Run all cells in order**\n",
    "\n",
    "4. **Models save to:** `../ml_models/`\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Important Notes\n",
    "\n",
    "- **GPU is REQUIRED** - CPU training will be extremely slow\n",
    "- **Free Colab has limits** - You get ~12 hours of GPU time per day\n",
    "- **Training 200 stocks** takes ~30-60 minutes with T4 GPU\n",
    "- **Increase stocks** = longer training time (linear scaling)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceeed37",
   "metadata": {},
   "source": [
    "## 1. Setup & Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4c36ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (UNCOMMENT if running in Colab)\n",
    "# !pip install torch pandas numpy yfinance joblib scikit-learn -q\n",
    "\n",
    "# Optional: Upload your stock list CSV in Colab\n",
    "# from google.colab import files\n",
    "# print(\"üìÅ Upload your eodhd_us_tickers.csv or stocks-list.csv file:\")\n",
    "# uploaded = files.upload()\n",
    "# # Move to data directory\n",
    "# import os\n",
    "# os.makedirs('data', exist_ok=True)\n",
    "# for filename in uploaded.keys():\n",
    "#     os.rename(filename, f'data/{filename}')\n",
    "#     print(f\"‚úÖ Uploaded {filename} to data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd4b2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"=\"*60)\n",
    "print(\"üîç GPU CHECK\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"‚úÖ GPU ENABLED: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(\"\\nüöÄ Training will be FAST!\")\n",
    "else:\n",
    "    print(\"‚ùå GPU NOT AVAILABLE - Training will be VERY SLOW!\")\n",
    "    print(\"\\nüí° If you're in Google Colab:\")\n",
    "    print(\"   1. Runtime ‚Üí Change runtime type\")\n",
    "    print(\"   2. Hardware accelerator ‚Üí GPU\")\n",
    "    print(\"   3. Click Save\")\n",
    "    print(\"   4. Re-run this cell\")\n",
    "    print(\"\\n‚ö†Ô∏è  Consider enabling GPU before proceeding!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2a7e92",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1094f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'training_universe_size': None,  # Smaller for faster LSTM training\n",
    "    'sequence_length': 60,          # 60 days of history per sequence\n",
    "    'forward_prediction_days': 30,  # Predict 30 days ahead\n",
    "    'hidden_size': 128,             # LSTM hidden units\n",
    "    'num_layers': 2,                # LSTM layers\n",
    "    'dropout': 0.2,\n",
    "    'batch_size': 64,\n",
    "    'epochs': 50,\n",
    "    'learning_rate': 0.001,\n",
    "    'output_dir': 'ml_models',\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5403f89f",
   "metadata": {},
   "source": [
    "## 3. Load Stock Universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4708cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stock list from CSV or download S&P 500\n",
    "import os\n",
    "\n",
    "try:\n",
    "    # Try multiple possible paths\n",
    "    for path in ['../data/eodhd_us_tickers.csv', 'data/eodhd_us_tickers.csv',\n",
    "                 '../data/stocks-list.csv', 'data/stocks-list.csv']:\n",
    "        if os.path.exists(path):\n",
    "            stocks_df = pd.read_csv(path)\n",
    "            # Try different column names\n",
    "            for col in ['Symbol', 'symbol', 'ticker', 'Ticker', 'SYMBOL']:\n",
    "                if col in stocks_df.columns:\n",
    "                    universe = stocks_df[col].head(CONFIG['training_universe_size']).tolist()\n",
    "                    print(f\"‚úÖ Loaded {len(universe)} stocks from {path}\")\n",
    "                    break\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No CSV found\")\n",
    "except:\n",
    "    # Fallback: Download S&P 500 stocks\n",
    "    print(\"‚ö†Ô∏è  CSV not found, downloading S&P 500 list...\")\n",
    "    sp500_table = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
    "    universe = sp500_table['Symbol'].head(CONFIG['training_universe_size']).tolist()\n",
    "    print(f\"‚úÖ Loaded {len(universe)} stocks from S&P 500\")\n",
    "\n",
    "print(f\"Total stocks: {len(universe)}\")\n",
    "print(f\"First 10: {universe[:10]}\")\n",
    "print(f\"\\n‚è±Ô∏è  ESTIMATED TRAINING TIME:\")\n",
    "print(f\"   - 200 stocks: ~30-60 minutes with GPU\")\n",
    "print(f\"   - 1000 stocks: ~2-3 hours with GPU\")\n",
    "print(f\"   - 5000 stocks: ~8-12 hours with GPU\")\n",
    "print(f\"\\nüí° TIP: Training with {len(universe)} stocks will produce more robust models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f0d852",
   "metadata": {},
   "source": [
    "## 4. Download & Prepare Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dd0012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Cell 4 \"Download & Prepare Time Series Data\" with this:\n",
    "\n",
    "print(\"üìä Downloading historical data...\\n\")\n",
    "\n",
    "all_sequences = []\n",
    "all_labels = []\n",
    "failed_tickers = []\n",
    "\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=730)  # 2 years\n",
    "\n",
    "for i, symbol in enumerate(universe):\n",
    "    try:\n",
    "        ticker = yf.Ticker(symbol)\n",
    "        df = ticker.history(start=start_date, end=end_date)\n",
    "        \n",
    "        if len(df) < CONFIG['sequence_length'] + CONFIG['forward_prediction_days']:\n",
    "            failed_tickers.append(symbol)\n",
    "            continue\n",
    "        \n",
    "        # üîß FIX: Handle division by zero and extreme values\n",
    "        # Calculate returns with safety checks\n",
    "        df['return'] = df['Close'].pct_change()\n",
    "        \n",
    "        # Replace infinite returns (from zero prices)\n",
    "        df['return'] = df['return'].replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        # Clip extreme returns (>100% or <-100%)\n",
    "        df['return'] = df['return'].clip(-1.0, 1.0)\n",
    "        \n",
    "        # Fill NaN returns with 0\n",
    "        df['return'] = df['return'].fillna(0)\n",
    "        \n",
    "        # Normalize volume safely\n",
    "        volume_mean = df['Volume'].mean()\n",
    "        volume_std = df['Volume'].std()\n",
    "        \n",
    "        if volume_std > 0:\n",
    "            df['volume_norm'] = (df['Volume'] - volume_mean) / volume_std\n",
    "        else:\n",
    "            df['volume_norm'] = 0\n",
    "        \n",
    "        # Clip volume to reasonable range\n",
    "        df['volume_norm'] = df['volume_norm'].clip(-10, 10)\n",
    "        \n",
    "        # Create sequences\n",
    "        for j in range(CONFIG['sequence_length'], len(df) - CONFIG['forward_prediction_days']):\n",
    "            # Features: return and normalized volume\n",
    "            sequence = df[['return', 'volume_norm']].iloc[j-CONFIG['sequence_length']:j].values\n",
    "            \n",
    "            # Label: forward return\n",
    "            current_price = df['Close'].iloc[j]\n",
    "            future_price = df['Close'].iloc[j + CONFIG['forward_prediction_days']]\n",
    "            \n",
    "            # Safety check for label calculation\n",
    "            if current_price <= 0 or future_price <= 0:\n",
    "                continue\n",
    "            \n",
    "            label = (future_price / current_price) - 1\n",
    "            \n",
    "            # Clip label to reasonable range\n",
    "            label = np.clip(label, -1.0, 1.0)\n",
    "            \n",
    "            # Only add if no NaN or Inf\n",
    "            if not np.isnan(sequence).any() and not np.isinf(sequence).any() and not np.isnan(label) and not np.isinf(label):\n",
    "                all_sequences.append(sequence)\n",
    "                all_labels.append(label)\n",
    "        \n",
    "        if (i + 1) % 25 == 0:\n",
    "            print(f\"Processed {i+1}/{len(universe)} stocks, {len(all_sequences):,} sequences\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        failed_tickers.append(symbol)\n",
    "        if len(failed_tickers) <= 5:  # Print first 5 errors\n",
    "            print(f\"‚ö†Ô∏è  {symbol} failed: {str(e)[:100]}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n‚úÖ Data collection complete!\")\n",
    "print(f\"Total sequences: {len(all_sequences):,}\")\n",
    "print(f\"Failed tickers: {len(failed_tickers)}\")\n",
    "if failed_tickers:\n",
    "    print(f\"Failed tickers: {failed_tickers[:10]}...\" if len(failed_tickers) > 10 else f\"Failed tickers: {failed_tickers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498dbc18",
   "metadata": {},
   "source": [
    "## 5. Prepare PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac93406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Cell 5 \"Prepare PyTorch Dataset\" with this fixed version:\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(all_sequences)\n",
    "y = np.array(all_labels)\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "# üîß FIX: Clean data before scaling\n",
    "print(\"\\nüîç Checking for invalid values...\")\n",
    "\n",
    "# Check for NaN\n",
    "nan_mask_X = np.isnan(X)\n",
    "nan_mask_y = np.isnan(y)\n",
    "print(f\"NaN in X: {nan_mask_X.any()} ({nan_mask_X.sum()} values)\")\n",
    "print(f\"NaN in y: {nan_mask_y.any()} ({nan_mask_y.sum()} values)\")\n",
    "\n",
    "# Check for infinity\n",
    "inf_mask_X = np.isinf(X)\n",
    "inf_mask_y = np.isinf(y)\n",
    "print(f\"Inf in X: {inf_mask_X.any()} ({inf_mask_X.sum()} values)\")\n",
    "print(f\"Inf in y: {inf_mask_y.any()} ({inf_mask_y.sum()} values)\")\n",
    "\n",
    "# Remove sequences with NaN or Inf\n",
    "valid_mask = ~(nan_mask_X.any(axis=(1,2)) | inf_mask_X.any(axis=(1,2)) | nan_mask_y | inf_mask_y)\n",
    "X_clean = X[valid_mask]\n",
    "y_clean = y[valid_mask]\n",
    "\n",
    "print(f\"\\n‚úÖ Cleaned data:\")\n",
    "print(f\"   Original: {len(X):,} sequences\")\n",
    "print(f\"   After cleaning: {len(X_clean):,} sequences\")\n",
    "print(f\"   Removed: {len(X) - len(X_clean):,} invalid sequences ({(1 - len(X_clean)/len(X))*100:.2f}%)\")\n",
    "\n",
    "# Clip extreme values (handle outliers)\n",
    "print(\"\\nüîß Clipping extreme values...\")\n",
    "percentile_99 = np.percentile(np.abs(X_clean), 99)\n",
    "X_clean = np.clip(X_clean, -percentile_99, percentile_99)\n",
    "\n",
    "y_percentile_99 = np.percentile(np.abs(y_clean), 99)\n",
    "y_clean = np.clip(y_clean, -y_percentile_99, y_percentile_99)\n",
    "\n",
    "print(f\"   X clipped to [-{percentile_99:.4f}, {percentile_99:.4f}]\")\n",
    "print(f\"   y clipped to [-{y_percentile_99:.4f}, {y_percentile_99:.4f}]\")\n",
    "\n",
    "# Scale features\n",
    "print(\"\\nüîÑ Scaling features...\")\n",
    "scaler = StandardScaler()\n",
    "X_reshaped = X_clean.reshape(-1, X_clean.shape[-1])\n",
    "X_scaled = scaler.fit_transform(X_reshaped)\n",
    "X_scaled = X_scaled.reshape(X_clean.shape)\n",
    "\n",
    "# Final check\n",
    "if np.isnan(X_scaled).any() or np.isinf(X_scaled).any():\n",
    "    raise ValueError(\"‚ùå Scaling produced NaN or Inf values!\")\n",
    "else:\n",
    "    print(\"‚úÖ Scaling successful - no invalid values\")\n",
    "\n",
    "# Split train/test\n",
    "split_idx = int(0.8 * len(X_scaled))\n",
    "X_train = X_scaled[:split_idx]\n",
    "X_test = X_scaled[split_idx:]\n",
    "y_train = y_clean[:split_idx]\n",
    "y_test = y_clean[split_idx:]\n",
    "\n",
    "print(f\"\\nüìä Final dataset split:\")\n",
    "print(f\"   Training samples: {len(X_train):,}\")\n",
    "print(f\"   Testing samples: {len(X_test):,}\")\n",
    "\n",
    "# Create PyTorch datasets\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = StockDataset(X_train, y_train)\n",
    "test_dataset = StockDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "\n",
    "print(\"\\n‚úÖ PyTorch datasets created and ready for training!\")\n",
    "print(f\"   Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"   Training batches: {len(train_loader)}\")\n",
    "print(f\"   Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da69535",
   "metadata": {},
   "source": [
    "## 6. Define LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1be487",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super(StockLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # LSTM forward pass\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Use last time step\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Fully connected layer\n",
    "        output = self.fc(last_output)\n",
    "        \n",
    "        return output.squeeze()\n",
    "\n",
    "# Initialize model\n",
    "input_size = X.shape[2]  # Number of features\n",
    "model = StockLSTM(\n",
    "    input_size=input_size,\n",
    "    hidden_size=CONFIG['hidden_size'],\n",
    "    num_layers=CONFIG['num_layers'],\n",
    "    dropout=CONFIG['dropout']\n",
    ").to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])\n",
    "\n",
    "print(\"Model architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d3ec16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super(StockLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # LSTM forward pass\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Use last time step\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Fully connected layer\n",
    "        output = self.fc(last_output)\n",
    "        \n",
    "        return output.squeeze()\n",
    "\n",
    "# Initialize model\n",
    "input_size = X.shape[2]  # Number of features\n",
    "model = StockLSTM(\n",
    "    input_size=input_size,\n",
    "    hidden_size=CONFIG['hidden_size'],\n",
    "    num_layers=CONFIG['num_layers'],\n",
    "    dropout=CONFIG['dropout']\n",
    ").to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])\n",
    "\n",
    "print(\"Model architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75a187f",
   "metadata": {},
   "source": [
    "## 7. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581c5476",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Training LSTM model...\\n\")\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "for epoch in range(CONFIG['epochs']):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    # Save best model\n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        best_model_state = model.state_dict()\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{CONFIG['epochs']}] - Train Loss: {train_loss:.6f}, Test Loss: {test_loss:.6f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")\n",
    "print(f\"Best test loss: {best_test_loss:.6f}\")\n",
    "\n",
    "# Restore best model\n",
    "model.load_state_dict(best_model_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d858096",
   "metadata": {},
   "source": [
    "## 8. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e09860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        all_preds.extend(outputs.cpu().numpy())\n",
    "        all_targets.extend(y_batch.numpy())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_targets = np.array(all_targets)\n",
    "\n",
    "mse = mean_squared_error(all_targets, all_preds)\n",
    "mae = mean_absolute_error(all_targets, all_preds)\n",
    "r2 = r2_score(all_targets, all_preds)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä LSTM MODEL PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"MSE:  {mse:.6f}\")\n",
    "print(f\"MAE:  {mae:.6f}\")\n",
    "print(f\"R¬≤ Score: {r2:.6f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3c8f85",
   "metadata": {},
   "source": [
    "## 9. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2a70c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_dir = f\"../{CONFIG['output_dir']}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"üíæ Saving model to {output_dir}/...\\n\")\n",
    "\n",
    "# Save PyTorch model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'input_size': input_size,\n",
    "    'hidden_size': CONFIG['hidden_size'],\n",
    "    'num_layers': CONFIG['num_layers'],\n",
    "    'dropout': CONFIG['dropout'],\n",
    "    'sequence_length': CONFIG['sequence_length'],\n",
    "}, f\"{output_dir}/lstm.pth\")\n",
    "print(\"‚úÖ Saved lstm.pth\")\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, f\"{output_dir}/lstm_scaler.pkl\")\n",
    "print(\"‚úÖ Saved lstm_scaler.pkl\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'num_stocks': len(universe) - len(failed_tickers),\n",
    "    'training_samples': len(X_train),\n",
    "    'test_samples': len(X_test),\n",
    "    'sequence_length': CONFIG['sequence_length'],\n",
    "    'input_size': input_size,\n",
    "    'performance': {\n",
    "        'mse': float(mse),\n",
    "        'mae': float(mae),\n",
    "        'r2': float(r2),\n",
    "        'best_test_loss': float(best_test_loss)\n",
    "    },\n",
    "    'config': CONFIG\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir}/lstm_metadata.json\", 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(\"‚úÖ Saved lstm_metadata.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ LSTM TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nFiles saved to: {output_dir}/\")\n",
    "print(\"  - lstm.pth\")\n",
    "print(\"  - lstm_scaler.pkl\")\n",
    "print(\"  - lstm_metadata.json\")\n",
    "print(\"\\nReady to deploy! üöÄ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac068716",
   "metadata": {},
   "source": [
    "## 10. Download Models (For Colab Users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cfea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download models for Colab (UNCOMMENT if running in Colab)\n",
    "# from google.colab import files\n",
    "# import zipfile\n",
    "# import os\n",
    "\n",
    "# print(\"üì¶ Creating zip file with LSTM models...\")\n",
    "\n",
    "# # Create zip file\n",
    "# zip_filename = 'lstm_models.zip'\n",
    "# with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "#     for file in ['lstm.pth', 'lstm_scaler.pkl', 'lstm_metadata.json']:\n",
    "#         file_path = f\"{output_dir}/{file}\"\n",
    "#         if os.path.exists(file_path):\n",
    "#             zipf.write(file_path, file)\n",
    "#             print(f\"  ‚úÖ Added {file}\")\n",
    "#         else:\n",
    "#             print(f\"  ‚ö†Ô∏è  {file} not found\")\n",
    "\n",
    "# print(f\"\\n‚¨áÔ∏è  Downloading {zip_filename}...\")\n",
    "# files.download(zip_filename)\n",
    "# print(\"‚úÖ Download complete!\")\n",
    "# print(\"\\nüìã Next steps:\")\n",
    "# print(\"1. Extract the zip file\")\n",
    "# print(\"2. Upload lstm.pth and lstm_scaler.pkl to your project's ml_models/ directory\")\n",
    "# print(\"3. The LSTM model adds 10% weight to ensemble predictions for Premium tier\")\n",
    "# print(\"4. Make sure to train ensemble models first (train_ensemble_models.ipynb)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
