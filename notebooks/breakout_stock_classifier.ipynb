{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca0b1067",
   "metadata": {},
   "source": [
    "# Breakout Stock Classifier: Scaffolding and Expansion\n",
    "\n",
    "This notebook scaffolds a modular backend for a breakout stock classifier, breaks out model components, adds data point functionality, displays and edits the training cell, and updates the workflow to handle more stocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27461137",
   "metadata": {},
   "source": [
    "## Google Colab: Uploading Your CSV\n",
    "\n",
    "If you are using Google Colab, you can upload your `stocks-list.csv` file directly to the Colab runtime with the following code cell:\n",
    "\n",
    "```python\n",
    "from google.colab import files\n",
    "uploaded = files.upload()  # This will prompt you to select and upload your CSV file\n",
    "```\n",
    "\n",
    "- After uploading, the file will be in the current working directory.\n",
    "- If your code expects the file in a `data/` folder, move it with:\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.replace('stocks-list.csv', 'data/stocks-list.csv')\n",
    "```\n",
    "\n",
    "Alternatively, you can mount your Google Drive and access files from there:\n",
    "\n",
    "```python\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# Then use the path: '/content/drive/My Drive/path/to/stocks-list.csv'\n",
    "```\n",
    "\n",
    "Adjust your code to use the correct path depending on your upload method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca445b9",
   "metadata": {},
   "source": [
    "## 1. Scaffold Model Backend\n",
    "Set up the basic backend structure for the breakout classifier, including imports and class/function definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9db6896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and backend scaffolding\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "\n",
    "# Placeholder for backend class\n",
    "class BreakoutStockClassifier:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.features = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.model = XGBClassifier(n_estimators=200, max_depth=5, random_state=42)\n",
    "        self.model.fit(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict_proba(X)[:, 1] if self.model else None\n",
    "    \n",
    "    def save(self, path):\n",
    "        joblib.dump(self.model, path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        self.model = joblib.load(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c6826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EOD Historical Data: Download US stock price data (replace Alpha Vantage) ---\n",
    "\n",
    "from eodhd import APIClient\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Use your actual EODHD API token here\n",
    "EODHD_API_KEY = \"68ebce6775f004.44089353\"\n",
    "\n",
    "def download_eodhd_bulk(tickers, api_key=EODHD_API_KEY, start_date=\"2015-01-01\", end_date=None, batch_size=5, delay=2):\n",
    "    \"\"\"\n",
    "    Download daily historical data for a list of tickers from EODHD.\n",
    "    Returns a DataFrame with all data concatenated.\n",
    "    Ensures 'close' column exists for all tickers.\n",
    "    \"\"\"\n",
    "    client = APIClient(api_key)\n",
    "    all_data = []\n",
    "    total = len(tickers)\n",
    "    for i in range(0, total, batch_size):\n",
    "        batch = tickers[i:i+batch_size]\n",
    "        print(f\"Downloading batch {i//batch_size+1} ({i+1}-{min(i+batch_size, total)}) of {total}...\")\n",
    "        for ticker in batch:\n",
    "            try:\n",
    "                df = client.get_historical_data(\n",
    "                    symbol=ticker,\n",
    "                    interval=\"d\",\n",
    "                    iso8601_start=start_date,\n",
    "                    iso8601_end=end_date if end_date else \"\"\n",
    "                )\n",
    "                if not df.empty:\n",
    "                    if 'close' not in df.columns:\n",
    "                        print(f\"Ticker {ticker} missing 'close' column, adding as NaN.\")\n",
    "                        df['close'] = np.nan\n",
    "                    df['symbol'] = ticker\n",
    "                    all_data.append(df)\n",
    "                else:\n",
    "                    print(f\"No data for {ticker}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {ticker}: {e}\")\n",
    "        if i + batch_size < total:\n",
    "            print(f\"Sleeping for {delay} seconds to avoid API rate limits...\")\n",
    "            time.sleep(delay)\n",
    "    if all_data:\n",
    "        return pd.concat(all_data, ignore_index=True)\n",
    "    else:\n",
    "        print(\"No valid data downloaded.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Example usage:\n",
    "# us_tickers = [\"AAPL\", \"MSFT\", \"GOOG\"]\n",
    "# df = download_eodhd_bulk(us_tickers)\n",
    "# print(df.head())\n",
    "\n",
    "# Remove Alpha Vantage import and function\n",
    "def download_alpha_vantage_bulk(*args, **kwargs):\n",
    "    raise NotImplementedError(\"Alpha Vantage integration has been replaced by EODHD.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95b26c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Download and use a comprehensive ticker list from an external CSV ---\n",
    "import os\n",
    "\n",
    "def get_tickers_from_csv(csv_path: str) -> list:\n",
    "    \"\"\"Load a comprehensive list of tickers from an external CSV file.\"\"\"\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(csv_path)    # Accept common column names for tickers\n",
    "    for col in ['symbol', 'ticker', 'Ticker', 'SYMBOL', 'Symbol']:\n",
    "        if col in df.columns:\n",
    "            tickers = df[col].dropna().unique().tolist()\n",
    "            return tickers\n",
    "    raise ValueError(f\"No ticker column found in {csv_path}. Columns found: {df.columns.tolist()}\")\n",
    "\n",
    "# Example usage:\n",
    "# Download a full US stock list from NASDAQ, NYSE, AMEX, or use a third-party source like 'eodhistoricaldata.com', 'nasdaqtrader.com', or 'stockanalysis.com'.\n",
    "# Place the CSV in your data directory, e.g., 'data/all_us_tickers.csv'.\n",
    "# all_tickers = get_tickers_from_csv('data/all_us_tickers.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb7440c",
   "metadata": {},
   "source": [
    "## 2. Break Out Model Components\n",
    "Separate the workflow into modular functions for data loading, preprocessing, model definition, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d65af7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading function\n",
    "def load_stock_data(csv_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load historical stock data from CSV or other sources.\"\"\"\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "# Quality filter function\n",
    "def filter_quality_stocks(df: pd.DataFrame, min_price=5.0, min_volume=500000) -> pd.DataFrame:\n",
    "    \"\"\"Filter out low-quality stocks (penny stocks, illiquid stocks).\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Minimum price filter (avoid penny stocks)\n",
    "    df = df[df['close'] >= min_price]\n",
    "    \n",
    "    # Minimum volume filter (ensure liquidity)\n",
    "    df = df[df['volume'] >= min_volume]\n",
    "    \n",
    "    # Price stability check (avoid stocks with extreme volatility)\n",
    "    df['price_std_30d'] = df.groupby('symbol')['close'].transform(\n",
    "        lambda x: x.rolling(30, min_periods=1).std()\n",
    "    )\n",
    "    df['volatility'] = df['price_std_30d'] / df['close']\n",
    "    \n",
    "    # Filter out extremely volatile stocks (>50% daily volatility)\n",
    "    df = df[df['volatility'] < 0.5]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Calculate momentum features\n",
    "def calculate_momentum_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add momentum indicators to help identify sustained moves.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Sort by symbol and date\n",
    "    df = df.sort_values(['symbol', 'date'])\n",
    "    \n",
    "    # RSI (Relative Strength Index)\n",
    "    delta = df.groupby('symbol')['close'].diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(14).mean()\n",
    "    loss = -delta.where(delta < 0, 0).rolling(14).mean()\n",
    "    rs = gain / (loss + 1e-10)  # Avoid division by zero\n",
    "    df['rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # MACD\n",
    "    ema_12 = df.groupby('symbol')['close'].transform(lambda x: x.ewm(span=12, adjust=False).mean())\n",
    "    ema_26 = df.groupby('symbol')['close'].transform(lambda x: x.ewm(span=26, adjust=False).mean())\n",
    "    df['macd'] = ema_12 - ema_26\n",
    "    df['macd_signal'] = df.groupby('symbol')['macd'].transform(lambda x: x.ewm(span=9, adjust=False).mean())\n",
    "    df['macd_histogram'] = df['macd'] - df['macd_signal']\n",
    "    \n",
    "    # Rate of Change (10-day)\n",
    "    df['roc_10'] = (df.groupby('symbol')['close'].shift(0) / \n",
    "                    df.groupby('symbol')['close'].shift(10) - 1) * 100\n",
    "    \n",
    "    # Average True Range (volatility measure)\n",
    "    df['prev_close'] = df.groupby('symbol')['close'].shift(1)\n",
    "    df['tr'] = df[['high', 'low', 'prev_close']].apply(\n",
    "        lambda x: max(x['high'] - x['low'], \n",
    "                     abs(x['high'] - x['prev_close']) if pd.notna(x['prev_close']) else 0,\n",
    "                     abs(x['low'] - x['prev_close']) if pd.notna(x['prev_close']) else 0),\n",
    "        axis=1\n",
    "    )\n",
    "    df['atr'] = df.groupby('symbol')['tr'].transform(lambda x: x.rolling(14, min_periods=1).mean())\n",
    "    \n",
    "    # Volume ratio\n",
    "    df['avg_volume_30d'] = df.groupby('symbol')['volume'].transform(\n",
    "        lambda x: x.rolling(30, min_periods=1).mean()\n",
    "    )\n",
    "    df['volume_ratio'] = df['volume'] / (df['avg_volume_30d'] + 1)  # Avoid division by zero\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Preprocessing function with trend filtering and sustained breakout definition\n",
    "def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Feature engineering and breakout labeling with trend filtering.\n",
    "    Labels sustained breakouts (not just one-day spikes).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ensure all price/volume columns are numeric\n",
    "    for col in ['open', 'high', 'low', 'close', 'volume']:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Sort by symbol and date\n",
    "    df = df.sort_values(['symbol', 'date'])\n",
    "    \n",
    "    # Calculate moving averages for trend detection\n",
    "    df['sma_50'] = df.groupby('symbol')['close'].transform(lambda x: x.rolling(50, min_periods=1).mean())\n",
    "    df['sma_200'] = df.groupby('symbol')['close'].transform(lambda x: x.rolling(200, min_periods=1).mean())\n",
    "    \n",
    "    # Trend filter: Price must be above both 50 and 200 day MA\n",
    "    df['uptrend'] = (df['close'] > df['sma_50']) & (df['close'] > df['sma_200'])\n",
    "    \n",
    "    # Filter out downtrending stocks BEFORE labeling breakouts\n",
    "    print(f\"Before trend filter: {len(df)} rows\")\n",
    "    df = df[df['uptrend'] == True].copy()\n",
    "    print(f\"After trend filter (uptrend only): {len(df)} rows\")\n",
    "    \n",
    "    # Calculate multiple forward returns for sustained breakout detection\n",
    "    valid = (df['close'].notnull()) & (df['close'] != 0)\n",
    "    \n",
    "    for days in [30, 60, 90]:\n",
    "        col_name = f'forward_return_{days}d'\n",
    "        df[col_name] = np.nan\n",
    "        shifted = df.groupby('symbol')['close'].shift(-days)\n",
    "        df.loc[valid, col_name] = shifted[valid] / df['close'][valid] - 1\n",
    "    \n",
    "    # Calculate max drawdown during breakout period (30 days)\n",
    "    df['future_min_price'] = df.groupby('symbol')['close'].transform(\n",
    "        lambda x: x.rolling(30, min_periods=1).min().shift(-30)\n",
    "    )\n",
    "    df['max_drawdown_30d'] = (df['future_min_price'] / df['close']) - 1\n",
    "    \n",
    "    # Volume confirmation\n",
    "    df['volume_confirmed'] = df['volume_ratio'] > 1.2\n",
    "    \n",
    "    # Breakout definition: SUSTAINED growth across all periods + volume + no major drawdown\n",
    "    df['breakout'] = (\n",
    "        (df['forward_return_30d'] > 0.20) &   # Up 20%+ at 30 days\n",
    "        (df['forward_return_60d'] > 0.30) &   # Up 30%+ at 60 days\n",
    "        (df['forward_return_90d'] > 0.40) &   # Up 40%+ at 90 days\n",
    "        (df['max_drawdown_30d'] > -0.15) &    # No major crash (>15% drop)\n",
    "        (df['volume_confirmed'] == True)      # Volume above average\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Drop rows with any NaN in required feature columns\n",
    "    df = df.dropna(subset=['open', 'high', 'low', 'close', 'volume', 'rsi', 'macd', 'atr'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # ROC AUC\n",
    "    if len(np.unique(y_test)) > 1:\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        print(f\"ROC AUC: {auc:.3f}\")\n",
    "    else:\n",
    "        print(\"ROC AUC: N/A (only one class in test set)\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['No Breakout', 'Breakout']))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"True Negatives:  {cm[0,0]:>6}\")\n",
    "    print(f\"False Positives: {cm[0,1]:>6}\")\n",
    "    print(f\"False Negatives: {cm[1,0]:>6}\")\n",
    "    print(f\"True Positives:  {cm[1,1]:>6}\")\n",
    "    \n",
    "    # Breakout prediction stats\n",
    "    print(f\"\\nBreakout predictions: {y_pred.sum()} out of {len(y_pred)} ({y_pred.sum()/len(y_pred)*100:.1f}%)\")\n",
    "    print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d4bdf9",
   "metadata": {},
   "source": [
    "## 3. Add Data Point Functionality\n",
    "Implement a function to add or update individual data points for training or testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4819ba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add or update a data point\n",
    "def add_data_point(df: pd.DataFrame, new_row: Dict) -> pd.DataFrame:\n",
    "    \"\"\"Add or update a single data point in the DataFrame.\"\"\"\n",
    "    df = df.copy()\n",
    "    # Assume 'date' and 'symbol' uniquely identify a row\n",
    "    mask = (df['date'] == new_row['date']) & (df['symbol'] == new_row['symbol'])\n",
    "    if mask.any():\n",
    "        df.loc[mask, :] = pd.DataFrame([new_row])\n",
    "    else:\n",
    "        df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e76693c",
   "metadata": {},
   "source": [
    "## 4. Display and Edit Training Cell\n",
    "This cell trains the breakout classifier. You can edit model parameters or code as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675009f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training cell: Improved breakout detection with trend filtering and sustained growth\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Load tickers from your CSV\n",
    "csv_path = Path('../data/stocks-list.csv')\n",
    "all_tickers = get_tickers_from_csv(str(csv_path))\n",
    "print(f\"‚úÖ Loaded {len(all_tickers)} tickers from {csv_path}\")\n",
    "\n",
    "# 2. Download historical price data (adjust slice as needed)\n",
    "print(\"\\nüìä Downloading historical data...\")\n",
    "bulk_df = download_eodhd_bulk(all_tickers[:500], start_date=\"2020-01-01\")  # 5 years of data\n",
    "\n",
    "# 3. Ensure columns are lowercase\n",
    "bulk_df.columns = [col.lower() for col in bulk_df.columns]\n",
    "print(f\"Downloaded {len(bulk_df)} total rows\")\n",
    "\n",
    "# 4. Check for required columns\n",
    "if 'close' not in bulk_df.columns:\n",
    "    raise ValueError(f\"'close' column not found. Columns: {bulk_df.columns.tolist()}\")\n",
    "\n",
    "# 5. Filter quality stocks first (remove penny stocks and low liquidity)\n",
    "print(\"\\nüîç Filtering quality stocks...\")\n",
    "bulk_df = filter_quality_stocks(bulk_df, min_price=5.0, min_volume=500000)\n",
    "print(f\"After quality filter: {len(bulk_df)} rows\")\n",
    "\n",
    "# 6. Add momentum features (RSI, MACD, volume ratio, etc.)\n",
    "print(\"\\nüìà Calculating momentum features...\")\n",
    "bulk_df = calculate_momentum_features(bulk_df)\n",
    "\n",
    "# 7. Preprocess with trend filter and sustained breakout definition\n",
    "print(\"\\nüéØ Labeling sustained breakouts...\")\n",
    "bulk_df = preprocess_data(bulk_df)\n",
    "print(f\"After preprocessing: {len(bulk_df)} rows\")\n",
    "print(f\"Breakout rate: {bulk_df['breakout'].mean():.2%} ({bulk_df['breakout'].sum()} breakouts)\")\n",
    "\n",
    "# 8. Check if we have enough breakouts to train\n",
    "if bulk_df['breakout'].sum() < 10:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Very few breakouts found. Consider:\")\n",
    "    print(\"   - Lowering return thresholds\")\n",
    "    print(\"   - Downloading more stocks\")\n",
    "    print(\"   - Using more historical data\")\n",
    "\n",
    "# 9. Select features and target\n",
    "features = ['open', 'high', 'low', 'close', 'volume', \n",
    "            'sma_50', 'sma_200', 'rsi', 'macd', 'macd_histogram', \n",
    "            'roc_10', 'atr', 'volume_ratio']\n",
    "\n",
    "X = bulk_df[features].fillna(0)  # Handle any remaining NaNs\n",
    "y = bulk_df['breakout']\n",
    "\n",
    "print(f\"\\nüìä Training data shape: {X.shape}\")\n",
    "print(f\"Features: {features}\")\n",
    "\n",
    "# 10. Train/test split with stratification (if possible)\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    print(f\"Train set: {len(X_train)} samples ({y_train.sum()} breakouts)\")\n",
    "    print(f\"Test set: {len(X_test)} samples ({y_test.sum()} breakouts)\")\n",
    "except ValueError as e:\n",
    "    print(f\"‚ö†Ô∏è  Cannot stratify (likely too few breakouts): {e}\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "# 11. Train model with class imbalance handling\n",
    "print(\"\\nüöÄ Training XGBoost classifier...\")\n",
    "clf = BreakoutStockClassifier()\n",
    "\n",
    "# Calculate scale_pos_weight to handle imbalanced data\n",
    "pos_count = y_train.sum()\n",
    "neg_count = len(y_train) - pos_count\n",
    "scale_pos_weight = neg_count / pos_count if pos_count > 0 else 1\n",
    "\n",
    "clf.model = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"‚úÖ Training complete!\")\n",
    "\n",
    "# 12. Evaluate model\n",
    "evaluate_model(clf.model, X_test, y_test)\n",
    "\n",
    "# 13. Feature importance\n",
    "print(\"\\nüîù Top 10 Most Important Features:\")\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': clf.model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "print(feature_importance.head(10).to_string(index=False))\n",
    "\n",
    "# 14. Save model\n",
    "model_path = '../ml_models/breakout_classifier_xgb.pkl'\n",
    "clf.save(model_path)\n",
    "print(f\"\\nüíæ Model saved to: {model_path}\")\n",
    "print(\"\\nüéâ Training complete! Model finds SUSTAINED breakouts in UPTRENDING stocks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1c9557",
   "metadata": {},
   "source": [
    "## 5. Improved Breakout Detection\n",
    "\n",
    "This updated notebook now finds **real, sustained breakouts** instead of one-day spikes:\n",
    "\n",
    "### Key Improvements:\n",
    "\n",
    "1. **Trend Filter**: Only considers stocks in uptrends (price above 50 & 200-day MA)\n",
    "2. **Sustained Returns**: Requires positive returns at 30, 60, AND 90 days:\n",
    "   - 20%+ at 30 days\n",
    "   - 30%+ at 60 days  \n",
    "   - 40%+ at 90 days\n",
    "3. **Volume Confirmation**: Breakouts need above-average volume (1.2x+)\n",
    "4. **Quality Filter**: Excludes penny stocks (<$5) and illiquid stocks (<500K volume)\n",
    "5. **Drawdown Protection**: Excludes stocks that crash >15% mid-period\n",
    "6. **Momentum Features**: RSI, MACD, ROC help identify real momentum\n",
    "\n",
    "### Expected Results:\n",
    "- Fewer breakout labels (more selective)\n",
    "- Higher quality picks that sustain growth\n",
    "- Better for real trading/investment decisions\n",
    "\n",
    "### If you get too few breakouts:\n",
    "- Lower the return thresholds (e.g., 15%/25%/35%)\n",
    "- Increase the stock universe (use more tickers)\n",
    "- Use more historical data (start_date=\"2015-01-01\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
