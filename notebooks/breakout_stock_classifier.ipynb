{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca0b1067",
   "metadata": {},
   "source": [
    "# Breakout Stock Classifier: Scaffolding and Expansion\n",
    "\n",
    "This notebook scaffolds a modular backend for a breakout stock classifier, breaks out model components, adds data point functionality, displays and edits the training cell, and updates the workflow to handle more stocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27461137",
   "metadata": {},
   "source": [
    "## Google Colab: Uploading Your CSV\n",
    "\n",
    "If you are using Google Colab, you can upload your `stocks-list.csv` file directly to the Colab runtime with the following code cell:\n",
    "\n",
    "```python\n",
    "from google.colab import files\n",
    "uploaded = files.upload()  # This will prompt you to select and upload your CSV file\n",
    "```\n",
    "\n",
    "- After uploading, the file will be in the current working directory.\n",
    "- If your code expects the file in a `data/` folder, move it with:\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.replace('stocks-list.csv', 'data/stocks-list.csv')\n",
    "```\n",
    "\n",
    "Alternatively, you can mount your Google Drive and access files from there:\n",
    "\n",
    "```python\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# Then use the path: '/content/drive/My Drive/path/to/stocks-list.csv'\n",
    "```\n",
    "\n",
    "Adjust your code to use the correct path depending on your upload method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca445b9",
   "metadata": {},
   "source": [
    "## 1. Scaffold Model Backend\n",
    "Set up the basic backend structure for the breakout classifier, including imports and class/function definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9db6896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and backend scaffolding\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "\n",
    "# Placeholder for backend class\n",
    "class BreakoutStockClassifier:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.features = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.model = XGBClassifier(n_estimators=200, max_depth=5, random_state=42)\n",
    "        self.model.fit(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict_proba(X)[:, 1] if self.model else None\n",
    "    \n",
    "    def save(self, path):\n",
    "        joblib.dump(self.model, path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        self.model = joblib.load(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c6826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EOD Historical Data: Download US stock price data (replace Alpha Vantage) ---\n",
    "\n",
    "from eodhd import APIClient\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Use your actual EODHD API token here\n",
    "EODHD_API_KEY = \"68ebce6775f004.44089353\"\n",
    "\n",
    "def download_eodhd_bulk(tickers, api_key=EODHD_API_KEY, start_date=\"2015-01-01\", end_date=None, batch_size=5, delay=2):\n",
    "    \"\"\"\n",
    "    Download daily historical data for a list of tickers from EODHD.\n",
    "    Returns a DataFrame with all data concatenated.\n",
    "    Ensures 'close' column exists for all tickers.\n",
    "    \"\"\"\n",
    "    client = APIClient(api_key)\n",
    "    all_data = []\n",
    "    total = len(tickers)\n",
    "    for i in range(0, total, batch_size):\n",
    "        batch = tickers[i:i+batch_size]\n",
    "        print(f\"Downloading batch {i//batch_size+1} ({i+1}-{min(i+batch_size, total)}) of {total}...\")\n",
    "        for ticker in batch:\n",
    "            try:\n",
    "                df = client.get_historical_data(\n",
    "                    symbol=ticker,\n",
    "                    interval=\"d\",\n",
    "                    iso8601_start=start_date,\n",
    "                    iso8601_end=end_date if end_date else \"\"\n",
    "                )\n",
    "                if not df.empty:\n",
    "                    if 'close' not in df.columns:\n",
    "                        print(f\"Ticker {ticker} missing 'close' column, adding as NaN.\")\n",
    "                        df['close'] = np.nan\n",
    "                    df['symbol'] = ticker\n",
    "                    all_data.append(df)\n",
    "                else:\n",
    "                    print(f\"No data for {ticker}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {ticker}: {e}\")\n",
    "        if i + batch_size < total:\n",
    "            print(f\"Sleeping for {delay} seconds to avoid API rate limits...\")\n",
    "            time.sleep(delay)\n",
    "    if all_data:\n",
    "        return pd.concat(all_data, ignore_index=True)\n",
    "    else:\n",
    "        print(\"No valid data downloaded.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Example usage:\n",
    "# us_tickers = [\"AAPL\", \"MSFT\", \"GOOG\"]\n",
    "# df = download_eodhd_bulk(us_tickers)\n",
    "# print(df.head())\n",
    "\n",
    "# Remove Alpha Vantage import and function\n",
    "def download_alpha_vantage_bulk(*args, **kwargs):\n",
    "    raise NotImplementedError(\"Alpha Vantage integration has been replaced by EODHD.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95b26c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Download and use a comprehensive ticker list from an external CSV ---\n",
    "import os\n",
    "\n",
    "def get_tickers_from_csv(csv_path: str) -> list:\n",
    "    \"\"\"Load a comprehensive list of tickers from an external CSV file.\"\"\"\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(csv_path)    # Accept common column names for tickers\n",
    "    for col in ['symbol', 'ticker', 'Ticker', 'SYMBOL', 'Symbol']:\n",
    "        if col in df.columns:\n",
    "            tickers = df[col].dropna().unique().tolist()\n",
    "            return tickers\n",
    "    raise ValueError(f\"No ticker column found in {csv_path}. Columns found: {df.columns.tolist()}\")\n",
    "\n",
    "# Example usage:\n",
    "# Download a full US stock list from NASDAQ, NYSE, AMEX, or use a third-party source like 'eodhistoricaldata.com', 'nasdaqtrader.com', or 'stockanalysis.com'.\n",
    "# Place the CSV in your data directory, e.g., 'data/all_us_tickers.csv'.\n",
    "# all_tickers = get_tickers_from_csv('data/all_us_tickers.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb7440c",
   "metadata": {},
   "source": [
    "## 2. Break Out Model Components\n",
    "Separate the workflow into modular functions for data loading, preprocessing, model definition, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d65af7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading function\n",
    "def load_stock_data(csv_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load historical stock data from CSV or other sources.\"\"\"\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "# Quality filter function\n",
    "def filter_quality_stocks(df: pd.DataFrame, min_price=5.0, min_volume=500000) -> pd.DataFrame:\n",
    "    \"\"\"Filter out low-quality stocks (penny stocks, illiquid stocks).\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Minimum price filter (avoid penny stocks)\n",
    "    df = df[df['close'] >= min_price]\n",
    "    \n",
    "    # Minimum volume filter (ensure liquidity)\n",
    "    df = df[df['volume'] >= min_volume]\n",
    "    \n",
    "    # Price stability check (avoid stocks with extreme volatility)\n",
    "    df['price_std_30d'] = df.groupby('symbol')['close'].transform(\n",
    "        lambda x: x.rolling(30, min_periods=1).std()\n",
    "    )\n",
    "    df['volatility'] = df['price_std_30d'] / df['close']\n",
    "    \n",
    "    # Filter out extremely volatile stocks (>50% daily volatility)\n",
    "    df = df[df['volatility'] < 0.5]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Calculate momentum features\n",
    "def calculate_momentum_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add momentum indicators to help identify sustained moves.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Sort by symbol and date\n",
    "    df = df.sort_values(['symbol', 'date'])\n",
    "    \n",
    "    # RSI (Relative Strength Index)\n",
    "    delta = df.groupby('symbol')['close'].diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(14).mean()\n",
    "    loss = -delta.where(delta < 0, 0).rolling(14).mean()\n",
    "    rs = gain / (loss + 1e-10)  # Avoid division by zero\n",
    "    df['rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # MACD\n",
    "    ema_12 = df.groupby('symbol')['close'].transform(lambda x: x.ewm(span=12, adjust=False).mean())\n",
    "    ema_26 = df.groupby('symbol')['close'].transform(lambda x: x.ewm(span=26, adjust=False).mean())\n",
    "    df['macd'] = ema_12 - ema_26\n",
    "    df['macd_signal'] = df.groupby('symbol')['macd'].transform(lambda x: x.ewm(span=9, adjust=False).mean())\n",
    "    df['macd_histogram'] = df['macd'] - df['macd_signal']\n",
    "    \n",
    "    # Rate of Change (10-day)\n",
    "    df['roc_10'] = (df.groupby('symbol')['close'].shift(0) / \n",
    "                    df.groupby('symbol')['close'].shift(10) - 1) * 100\n",
    "    \n",
    "    # Average True Range (volatility measure)\n",
    "    df['prev_close'] = df.groupby('symbol')['close'].shift(1)\n",
    "    df['tr'] = df[['high', 'low', 'prev_close']].apply(\n",
    "        lambda x: max(x['high'] - x['low'], \n",
    "                     abs(x['high'] - x['prev_close']) if pd.notna(x['prev_close']) else 0,\n",
    "                     abs(x['low'] - x['prev_close']) if pd.notna(x['prev_close']) else 0),\n",
    "        axis=1\n",
    "    )\n",
    "    df['atr'] = df.groupby('symbol')['tr'].transform(lambda x: x.rolling(14, min_periods=1).mean())\n",
    "    \n",
    "    # Volume ratio\n",
    "    df['avg_volume_30d'] = df.groupby('symbol')['volume'].transform(\n",
    "        lambda x: x.rolling(30, min_periods=1).mean()\n",
    "    )\n",
    "    df['volume_ratio'] = df['volume'] / (df['avg_volume_30d'] + 1)  # Avoid division by zero\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Preprocessing function with trend filtering and sustained breakout definition\n",
    "def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Feature engineering and breakout labeling with trend filtering.\n",
    "    Labels sustained breakouts (not just one-day spikes).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ensure all price/volume columns are numeric\n",
    "    for col in ['open', 'high', 'low', 'close', 'volume']:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Sort by symbol and date\n",
    "    df = df.sort_values(['symbol', 'date'])\n",
    "    \n",
    "    # Calculate moving averages for trend detection\n",
    "    df['sma_50'] = df.groupby('symbol')['close'].transform(lambda x: x.rolling(50, min_periods=1).mean())\n",
    "    df['sma_200'] = df.groupby('symbol')['close'].transform(lambda x: x.rolling(200, min_periods=1).mean())\n",
    "    \n",
    "    # Trend filter: Price must be above both 50 and 200 day MA\n",
    "    df['uptrend'] = (df['close'] > df['sma_50']) & (df['close'] > df['sma_200'])\n",
    "    \n",
    "    # Filter out downtrending stocks BEFORE labeling breakouts\n",
    "    print(f\"Before trend filter: {len(df)} rows\")\n",
    "    df = df[df['uptrend'] == True].copy()\n",
    "    print(f\"After trend filter (uptrend only): {len(df)} rows\")\n",
    "    \n",
    "    # Calculate multiple forward returns for sustained breakout detection\n",
    "    valid = (df['close'].notnull()) & (df['close'] != 0)\n",
    "    \n",
    "    for days in [30, 60, 90]:\n",
    "        col_name = f'forward_return_{days}d'\n",
    "        df[col_name] = np.nan\n",
    "        shifted = df.groupby('symbol')['close'].shift(-days)\n",
    "        df.loc[valid, col_name] = shifted[valid] / df['close'][valid] - 1\n",
    "    \n",
    "    # Calculate max drawdown during breakout period (30 days)\n",
    "    df['future_min_price'] = df.groupby('symbol')['close'].transform(\n",
    "        lambda x: x.rolling(30, min_periods=1).min().shift(-30)\n",
    "    )\n",
    "    df['max_drawdown_30d'] = (df['future_min_price'] / df['close']) - 1\n",
    "    \n",
    "    # Volume confirmation\n",
    "    df['volume_confirmed'] = df['volume_ratio'] > 1.2\n",
    "    \n",
    "    # Breakout definition: SUSTAINED growth across all periods + volume + no major drawdown\n",
    "    df['breakout'] = (\n",
    "        (df['forward_return_30d'] > 0.20) &   # Up 20%+ at 30 days\n",
    "        (df['forward_return_60d'] > 0.30) &   # Up 30%+ at 60 days\n",
    "        (df['forward_return_90d'] > 0.40) &   # Up 40%+ at 90 days\n",
    "        (df['max_drawdown_30d'] > -0.15) &    # No major crash (>15% drop)\n",
    "        (df['volume_confirmed'] == True)      # Volume above average\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Drop rows with any NaN in required feature columns\n",
    "    df = df.dropna(subset=['open', 'high', 'low', 'close', 'volume', 'rsi', 'macd', 'atr'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # ROC AUC\n",
    "    if len(np.unique(y_test)) > 1:\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        print(f\"ROC AUC: {auc:.3f}\")\n",
    "    else:\n",
    "        print(\"ROC AUC: N/A (only one class in test set)\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['No Breakout', 'Breakout']))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"True Negatives:  {cm[0,0]:>6}\")\n",
    "    print(f\"False Positives: {cm[0,1]:>6}\")\n",
    "    print(f\"False Negatives: {cm[1,0]:>6}\")\n",
    "    print(f\"True Positives:  {cm[1,1]:>6}\")\n",
    "    \n",
    "    # Breakout prediction stats\n",
    "    print(f\"\\nBreakout predictions: {y_pred.sum()} out of {len(y_pred)} ({y_pred.sum()/len(y_pred)*100:.1f}%)\")\n",
    "    print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d4bdf9",
   "metadata": {},
   "source": [
    "## 3. Add Data Point Functionality\n",
    "Implement a function to add or update individual data points for training or testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4819ba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add or update a data point\n",
    "def add_data_point(df: pd.DataFrame, new_row: Dict) -> pd.DataFrame:\n",
    "    \"\"\"Add or update a single data point in the DataFrame.\"\"\"\n",
    "    df = df.copy()\n",
    "    # Assume 'date' and 'symbol' uniquely identify a row\n",
    "    mask = (df['date'] == new_row['date']) & (df['symbol'] == new_row['symbol'])\n",
    "    if mask.any():\n",
    "        df.loc[mask, :] = pd.DataFrame([new_row])\n",
    "    else:\n",
    "        df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e76693c",
   "metadata": {},
   "source": [
    "## 4. Display and Edit Training Cell\n",
    "This cell trains the breakout classifier. You can edit model parameters or code as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675009f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training cell: Handle 5000+ stocks with batching and progress tracking\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'csv_path': '/stocks-list.csv',  # Colab uploaded file\n",
    "    'fallback_csv': '../data/stocks-list.csv',  # Local fallback\n",
    "    'start_date': '2020-01-01',  # 5 years of data\n",
    "    'batch_size': 100,  # Process 100 stocks at a time\n",
    "    'api_delay': 3,  # Seconds between API batches\n",
    "    'min_price': 5.0,\n",
    "    'min_volume': 500000,\n",
    "    'max_stocks': None,  # None = use all stocks from CSV\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üöÄ TRAINING BREAKOUT CLASSIFIER ON 5000+ STOCKS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Load tickers from CSV (try Colab path first)\n",
    "csv_path = CONFIG['csv_path'] if Path(CONFIG['csv_path']).exists() else CONFIG['fallback_csv']\n",
    "all_tickers = get_tickers_from_csv(csv_path)\n",
    "\n",
    "if CONFIG['max_stocks']:\n",
    "    all_tickers = all_tickers[:CONFIG['max_stocks']]\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(all_tickers)} tickers from {csv_path}\")\n",
    "print(f\"üìä Estimated download time: {(len(all_tickers) * CONFIG['api_delay'] / 60):.1f} minutes\")\n",
    "print(f\"‚è∞ Start time: {time.strftime('%H:%M:%S')}\")\n",
    "\n",
    "# 2. Download historical data in batches with progress tracking\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üì• DOWNLOADING HISTORICAL DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_dataframes = []\n",
    "failed_tickers = []\n",
    "total_rows = 0\n",
    "\n",
    "for batch_num, i in enumerate(range(0, len(all_tickers), CONFIG['batch_size']), 1):\n",
    "    batch_tickers = all_tickers[i:i + CONFIG['batch_size']]\n",
    "    progress = (i / len(all_tickers)) * 100\n",
    "    \n",
    "    print(f\"\\nüì¶ Batch {batch_num} ({i+1}-{min(i+CONFIG['batch_size'], len(all_tickers))} of {len(all_tickers)}) [{progress:.1f}%]\")\n",
    "    \n",
    "    try:\n",
    "        batch_df = download_eodhd_bulk(\n",
    "            batch_tickers,\n",
    "            start_date=CONFIG['start_date'],\n",
    "            batch_size=20,  # EODHD internal batch\n",
    "            delay=1  # Shorter delay within batch\n",
    "        )\n",
    "        \n",
    "        if not batch_df.empty:\n",
    "            # Ensure lowercase columns\n",
    "            batch_df.columns = [col.lower() for col in batch_df.columns]\n",
    "            all_dataframes.append(batch_df)\n",
    "            total_rows += len(batch_df)\n",
    "            print(f\"‚úÖ Downloaded {len(batch_df)} rows from {len(batch_df['symbol'].nunique())} stocks\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Batch {batch_num} returned no data\")\n",
    "            failed_tickers.extend(batch_tickers)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Batch {batch_num} failed: {e}\")\n",
    "        failed_tickers.extend(batch_tickers)\n",
    "    \n",
    "    # Progress update\n",
    "    print(f\"üìà Progress: {total_rows:,} rows | {len(all_dataframes)} batches | {len(failed_tickers)} failures\")\n",
    "    \n",
    "    # Rate limiting between batches\n",
    "    if i + CONFIG['batch_size'] < len(all_tickers):\n",
    "        print(f\"‚è∏Ô∏è  Cooling down for {CONFIG['api_delay']} seconds...\")\n",
    "        time.sleep(CONFIG['api_delay'])\n",
    "\n",
    "# 3. Combine all data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîó COMBINING DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if all_dataframes:\n",
    "    bulk_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    print(f\"‚úÖ Combined {len(bulk_df):,} total rows\")\n",
    "    print(f\"‚úÖ Unique stocks: {bulk_df['symbol'].nunique()}\")\n",
    "    print(f\"‚ö†Ô∏è  Failed tickers: {len(failed_tickers)}\")\n",
    "else:\n",
    "    raise ValueError(\"‚ùå No data downloaded successfully!\")\n",
    "\n",
    "# 4. Check for required columns\n",
    "if 'close' not in bulk_df.columns:\n",
    "    raise ValueError(f\"'close' column not found. Columns: {bulk_df.columns.tolist()}\")\n",
    "\n",
    "# 5. Filter quality stocks (parallel processing for speed)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîç FILTERING QUALITY STOCKS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Before filter: {len(bulk_df):,} rows\")\n",
    "\n",
    "bulk_df = filter_quality_stocks(\n",
    "    bulk_df,\n",
    "    min_price=CONFIG['min_price'],\n",
    "    min_volume=CONFIG['min_volume']\n",
    ")\n",
    "\n",
    "print(f\"After filter: {len(bulk_df):,} rows\")\n",
    "print(f\"Removed: {total_rows - len(bulk_df):,} low-quality data points\")\n",
    "print(f\"Remaining stocks: {bulk_df['symbol'].nunique()}\")\n",
    "\n",
    "# 6. Add momentum features (vectorized for speed)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìà CALCULATING MOMENTUM FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "bulk_df = calculate_momentum_features(bulk_df)\n",
    "calc_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚úÖ Calculated in {calc_time:.1f} seconds\")\n",
    "print(f\"Features added: RSI, MACD, ROC, ATR, Volume Ratio\")\n",
    "\n",
    "# 7. Preprocess with trend filter and sustained breakout definition\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ LABELING SUSTAINED BREAKOUTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "bulk_df = preprocess_data(bulk_df)\n",
    "\n",
    "print(f\"\\nüìä FINAL DATASET:\")\n",
    "print(f\"   Total rows: {len(bulk_df):,}\")\n",
    "print(f\"   Unique stocks: {bulk_df['symbol'].nunique()}\")\n",
    "print(f\"   Date range: {bulk_df['date'].min()} to {bulk_df['date'].max()}\")\n",
    "print(f\"   Breakouts found: {bulk_df['breakout'].sum():,}\")\n",
    "print(f\"   Breakout rate: {bulk_df['breakout'].mean():.2%}\")\n",
    "\n",
    "# 8. Validate sufficient training data\n",
    "if bulk_df['breakout'].sum() < 50:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Very few breakouts found (<50). Consider:\")\n",
    "    print(\"   - Lowering return thresholds in preprocess_data()\")\n",
    "    print(\"   - Using more historical data (start_date='2015-01-01')\")\n",
    "    print(\"   - Checking data quality\")\n",
    "elif bulk_df['breakout'].sum() < 200:\n",
    "    print(\"\\n‚ö†Ô∏è  CAUTION: Limited breakouts found (<200). Model may be less robust.\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Sufficient training data: {bulk_df['breakout'].sum()} breakouts\")\n",
    "\n",
    "# 9. Select features and prepare training data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üèóÔ∏è  PREPARING TRAINING DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "features = [\n",
    "    'open', 'high', 'low', 'close', 'volume',\n",
    "    'sma_50', 'sma_200', 'rsi', 'macd', 'macd_histogram',\n",
    "    'roc_10', 'atr', 'volume_ratio'\n",
    "]\n",
    "\n",
    "X = bulk_df[features].fillna(0)\n",
    "y = bulk_df['breakout']\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Features: {len(features)}\")\n",
    "print(f\"Class distribution:\")\n",
    "print(f\"   No Breakout: {(y==0).sum():,} ({(y==0).mean():.1%})\")\n",
    "print(f\"   Breakout:    {(y==1).sum():,} ({(y==1).mean():.1%})\")\n",
    "\n",
    "# 10. Train/test split with stratification\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÇÔ∏è  SPLITTING TRAIN/TEST DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    print(\"‚úÖ Stratified split successful\")\n",
    "except ValueError as e:\n",
    "    print(f\"‚ö†Ô∏è  Cannot stratify: {e}\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "print(f\"Train: {len(X_train):,} samples ({y_train.sum():,} breakouts, {y_train.mean():.2%})\")\n",
    "print(f\"Test:  {len(X_test):,} samples ({y_test.sum():,} breakouts, {y_test.mean():.2%})\")\n",
    "\n",
    "# 11. Train model with optimized hyperparameters\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ TRAINING XGBOOST CLASSIFIER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate class weight to handle imbalance\n",
    "pos_count = y_train.sum()\n",
    "neg_count = len(y_train) - pos_count\n",
    "scale_pos_weight = neg_count / pos_count if pos_count > 0 else 1\n",
    "\n",
    "print(f\"Scale pos weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "clf = BreakoutStockClassifier()\n",
    "clf.model = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    n_jobs=-1,  # Use all CPU cores\n",
    "    tree_method='hist'  # Faster for large datasets\n",
    ")\n",
    "\n",
    "train_start = time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "train_time = time.time() - train_start\n",
    "\n",
    "print(f\"‚úÖ Training complete in {train_time:.1f} seconds!\")\n",
    "\n",
    "# 12. Evaluate model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "evaluate_model(clf.model, X_test, y_test)\n",
    "\n",
    "# 13. Feature importance\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîù TOP 10 MOST IMPORTANT FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': clf.model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(feature_importance.head(10).to_string(index=False))\n",
    "\n",
    "# 14. Save model with metadata\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üíæ SAVING MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Ensure ml_models directory exists\n",
    "import os\n",
    "model_dir = '../python/ml_models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save model with descriptive name\n",
    "model_filename = f'breakout_classifier_xgb_stockslist.pkl'\n",
    "model_path = f'{model_dir}/{model_filename}'\n",
    "clf.save(model_path)\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'training_date': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'total_stocks': len(all_tickers),\n",
    "    'successful_stocks': bulk_df['symbol'].nunique(),\n",
    "    'failed_stocks': len(failed_tickers),\n",
    "    'training_samples': len(X_train),\n",
    "    'test_samples': len(X_test),\n",
    "    'breakout_rate': float(y.mean()),\n",
    "    'features': features,\n",
    "    'config': CONFIG,\n",
    "    'model_performance': {\n",
    "        'training_time_seconds': train_time,\n",
    "        'test_breakouts': int(y_test.sum())\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "metadata_path = f'{model_dir}/breakout_classifier_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {model_path}\")\n",
    "print(f\"‚úÖ Metadata saved to: {metadata_path}\")\n",
    "\n",
    "# 15. Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "elapsed = time.time() - time.time()\n",
    "print(f\"üìà Trained on {bulk_df['symbol'].nunique()} stocks\")\n",
    "print(f\"üéØ Found {bulk_df['breakout'].sum():,} sustained breakouts\")\n",
    "print(f\"‚úÖ Model finds SUSTAINED breakouts in UPTRENDING stocks\")\n",
    "print(f\"üìÅ Model location: {model_path}\")\n",
    "print(\"\\nüí° Next steps:\")\n",
    "print(\"   1. Download the model file from Colab\")\n",
    "print(\"   2. Upload to your python/ml_models/ directory\")\n",
    "print(\"   3. Use in your ML API for predictions\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1c9557",
   "metadata": {},
   "source": [
    "## 5. Improved Breakout Detection\n",
    "\n",
    "This updated notebook now finds **real, sustained breakouts** instead of one-day spikes:\n",
    "\n",
    "### Key Improvements:\n",
    "\n",
    "1. **Trend Filter**: Only considers stocks in uptrends (price above 50 & 200-day MA)\n",
    "2. **Sustained Returns**: Requires positive returns at 30, 60, AND 90 days:\n",
    "   - 20%+ at 30 days\n",
    "   - 30%+ at 60 days  \n",
    "   - 40%+ at 90 days\n",
    "3. **Volume Confirmation**: Breakouts need above-average volume (1.2x+)\n",
    "4. **Quality Filter**: Excludes penny stocks (<$5) and illiquid stocks (<500K volume)\n",
    "5. **Drawdown Protection**: Excludes stocks that crash >15% mid-period\n",
    "6. **Momentum Features**: RSI, MACD, ROC help identify real momentum\n",
    "\n",
    "### Expected Results:\n",
    "- Fewer breakout labels (more selective)\n",
    "- Higher quality picks that sustain growth\n",
    "- Better for real trading/investment decisions\n",
    "\n",
    "### If you get too few breakouts:\n",
    "- Lower the return thresholds (e.g., 15%/25%/35%)\n",
    "- Increase the stock universe (use more tickers)\n",
    "- Use more historical data (start_date=\"2015-01-01\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
