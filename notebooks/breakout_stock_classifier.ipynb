{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "890968cb",
   "metadata": {
    "papermill": {
     "duration": 0.004012,
     "end_time": "2025-11-29T05:26:47.060585",
     "exception": false,
     "start_time": "2025-11-29T05:26:47.056573",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Breakout Stock Classifier: Scaffolding and Expansion\n",
    "\n",
    "This notebook scaffolds a modular backend for a breakout stock classifier, breaks out model components, adds data point functionality, displays and edits the training cell, and updates the workflow to handle more stocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d49f6d6",
   "metadata": {
    "papermill": {
     "duration": 0.003196,
     "end_time": "2025-11-29T05:26:47.068315",
     "exception": false,
     "start_time": "2025-11-29T05:26:47.065119",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 0. Setup & Installation\n",
    "\n",
    "Run this cell first to install required packages (required for Kaggle/Colab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "397a0bf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T05:26:47.076991Z",
     "iopub.status.busy": "2025-11-29T05:26:47.076413Z",
     "iopub.status.idle": "2025-11-29T05:26:53.945461Z",
     "shell.execute_reply": "2025-11-29T05:26:53.944346Z"
    },
    "papermill": {
     "duration": 6.875057,
     "end_time": "2025-11-29T05:26:53.946963",
     "exception": false,
     "start_time": "2025-11-29T05:26:47.071906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install required packages (UNCOMMENT and run in Kaggle/Colab)\n",
    "!pip install eodhd xgboost scikit-learn pandas numpy joblib -q\n",
    "\n",
    "# Optional: Upload your stock list CSV in Kaggle/Colab\n",
    "# For Kaggle: Use \"Add Data\" button on the right panel instead\n",
    "# For Colab:\n",
    "# from google.colab import files\n",
    "# print(\"üìÅ Upload your stocks-list.csv file:\")\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17011cb",
   "metadata": {
    "papermill": {
     "duration": 0.00338,
     "end_time": "2025-11-29T05:26:53.953611",
     "exception": false,
     "start_time": "2025-11-29T05:26:53.950231",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Google Colab / Kaggle: Uploading Your CSV\n",
    "\n",
    "### For Google Colab:\n",
    "\n",
    "If you are using Google Colab, you can upload your `stocks-list.csv` file directly to the Colab runtime with the following code cell:\n",
    "\n",
    "```python\n",
    "from google.colab import files\n",
    "uploaded = files.upload()  # This will prompt you to select and upload your CSV file\n",
    "```\n",
    "\n",
    "- After uploading, the file will be in the current working directory.\n",
    "- If your code expects the file in a `data/` folder, move it with:\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.replace('stocks-list.csv', 'data/stocks-list.csv')\n",
    "```\n",
    "\n",
    "Alternatively, you can mount your Google Drive and access files from there:\n",
    "\n",
    "```python\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# Then use the path: '/content/drive/My Drive/path/to/stocks-list.csv'\n",
    "```\n",
    "\n",
    "### For Kaggle:\n",
    "\n",
    "1. **Upload your dataset to Kaggle:**\n",
    "   - Go to [kaggle.com/datasets](https://www.kaggle.com/datasets)\n",
    "   - Click \"New Dataset\" and upload `stocks-list.csv`\n",
    "   - Name it \"stocks-list\" (or similar)\n",
    "\n",
    "2. **Add the dataset to your notebook:**\n",
    "   - In your Kaggle notebook, click \"Add Data\" on the right panel\n",
    "   - Search for your uploaded dataset\n",
    "   - Add it to your notebook\n",
    "\n",
    "3. **The notebook will automatically detect Kaggle paths:**\n",
    "   - `/kaggle/input/stocks-list/stocks-list.csv`\n",
    "   - `/kaggle/input/stock-list/stocks-list.csv`\n",
    "\n",
    "4. **Output files** will be saved to `/kaggle/working/` and appear in the \"Output\" tab\n",
    "\n",
    "Adjust your code to use the correct path depending on your upload method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a104aa02",
   "metadata": {
    "papermill": {
     "duration": 0.003238,
     "end_time": "2025-11-29T05:26:53.960225",
     "exception": false,
     "start_time": "2025-11-29T05:26:53.956987",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Scaffold Model Backend\n",
    "Set up the basic backend structure for the breakout classifier, including imports and class/function definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9070d316",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T05:26:53.968635Z",
     "iopub.status.busy": "2025-11-29T05:26:53.968316Z",
     "iopub.status.idle": "2025-11-29T05:26:59.188256Z",
     "shell.execute_reply": "2025-11-29T05:26:59.187397Z"
    },
    "papermill": {
     "duration": 5.225929,
     "end_time": "2025-11-29T05:26:59.189731",
     "exception": false,
     "start_time": "2025-11-29T05:26:53.963802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports and backend scaffolding\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "\n",
    "# Placeholder for backend class\n",
    "class BreakoutStockClassifier:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.features = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.model = XGBClassifier(n_estimators=200, max_depth=5, random_state=42)\n",
    "        self.model.fit(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict_proba(X)[:, 1] if self.model else None\n",
    "    \n",
    "    def save(self, path):\n",
    "        joblib.dump(self.model, path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        self.model = joblib.load(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ea06558",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T05:26:59.199016Z",
     "iopub.status.busy": "2025-11-29T05:26:59.198557Z",
     "iopub.status.idle": "2025-11-29T05:26:59.692546Z",
     "shell.execute_reply": "2025-11-29T05:26:59.691747Z"
    },
    "papermill": {
     "duration": 0.500429,
     "end_time": "2025-11-29T05:26:59.693913",
     "exception": false,
     "start_time": "2025-11-29T05:26:59.193484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- EOD Historical Data: Download US stock price data (replace Alpha Vantage) ---\n",
    "\n",
    "from eodhd import APIClient\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Use your actual EODHD API token here\n",
    "EODHD_API_KEY = \"68ebce6775f004.44089353\"\n",
    "\n",
    "def download_eodhd_bulk(tickers, api_key=EODHD_API_KEY, start_date=\"2015-01-01\", end_date=None, batch_size=5, delay=2):\n",
    "    \"\"\"\n",
    "    Download daily historical data for a list of tickers from EODHD.\n",
    "    Returns a DataFrame with all data concatenated.\n",
    "    Ensures 'close' and 'date' columns exist for all tickers.\n",
    "    \"\"\"\n",
    "    # Defensive validation: ensure tickers is a list of strings\n",
    "    if tickers is None:\n",
    "        print(\"Warning: tickers is None, returning empty DataFrame\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    if not hasattr(tickers, '__iter__') or isinstance(tickers, (str, int, float)):\n",
    "        print(f\"Warning: tickers is not iterable (got {type(tickers).__name__}), returning empty DataFrame\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Convert to list and filter out non-string items\n",
    "    tickers = [t for t in tickers if isinstance(t, str) and t.strip()]\n",
    "    \n",
    "    if len(tickers) == 0:\n",
    "        print(\"Warning: No valid tickers provided, returning empty DataFrame\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    client = APIClient(api_key)\n",
    "    all_data = []\n",
    "    total = len(tickers)\n",
    "    for i in range(0, total, batch_size):\n",
    "        batch = tickers[i:i+batch_size]\n",
    "        print(f\"Downloading batch {i//batch_size+1} ({i+1}-{min(i+batch_size, total)}) of {total}...\")\n",
    "        for ticker in batch:\n",
    "            try:\n",
    "                df = client.get_historical_data(\n",
    "                    symbol=ticker,\n",
    "                    interval=\"d\",\n",
    "                    iso8601_start=start_date,\n",
    "                    iso8601_end=end_date if end_date else \"\"\n",
    "                )\n",
    "                if not df.empty:\n",
    "                    # Ensure 'date' column exists (EODHD may return date as index)\n",
    "                    if 'date' not in df.columns:\n",
    "                        if df.index.name == 'date' or df.index.name is None:\n",
    "                            df = df.reset_index()\n",
    "                            # Rename index column to 'date' if needed\n",
    "                            if 'index' in df.columns:\n",
    "                                df = df.rename(columns={'index': 'date'})\n",
    "                            elif df.columns[0] not in ['date', 'open', 'high', 'low', 'close', 'volume']:\n",
    "                                df = df.rename(columns={df.columns[0]: 'date'})\n",
    "                    \n",
    "                    # Ensure 'close' column exists\n",
    "                    if 'close' not in df.columns:\n",
    "                        print(f\"Ticker {ticker} missing 'close' column, adding as NaN.\")\n",
    "                        df['close'] = np.nan\n",
    "                    \n",
    "                    df['symbol'] = ticker\n",
    "                    all_data.append(df)\n",
    "                else:\n",
    "                    print(f\"No data for {ticker}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {ticker}: {e}\")\n",
    "        if i + batch_size < total:\n",
    "            print(f\"Sleeping for {delay} seconds to avoid API rate limits...\")\n",
    "            time.sleep(delay)\n",
    "    if all_data:\n",
    "        result_df = pd.concat(all_data, ignore_index=True)\n",
    "        # Final check: ensure 'date' column exists\n",
    "        if 'date' not in result_df.columns:\n",
    "            print(\"Warning: 'date' column still missing after concat, attempting to fix...\")\n",
    "            result_df = result_df.reset_index()\n",
    "            if 'index' in result_df.columns:\n",
    "                result_df = result_df.rename(columns={'index': 'date'})\n",
    "        return result_df\n",
    "    else:\n",
    "        print(\"No valid data downloaded.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Example usage:\n",
    "# us_tickers = [\"AAPL\", \"MSFT\", \"GOOG\"]\n",
    "# df = download_eodhd_bulk(us_tickers)\n",
    "# print(df.head())\n",
    "\n",
    "# Remove Alpha Vantage import and function\n",
    "def download_alpha_vantage_bulk(*args, **kwargs):\n",
    "    raise NotImplementedError(\"Alpha Vantage integration has been replaced by EODHD.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc1f5026",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T05:26:59.701922Z",
     "iopub.status.busy": "2025-11-29T05:26:59.701491Z",
     "iopub.status.idle": "2025-11-29T05:26:59.706581Z",
     "shell.execute_reply": "2025-11-29T05:26:59.706030Z"
    },
    "papermill": {
     "duration": 0.010535,
     "end_time": "2025-11-29T05:26:59.707940",
     "exception": false,
     "start_time": "2025-11-29T05:26:59.697405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Download and use a comprehensive ticker list from an external CSV ---\n",
    "import os\n",
    "\n",
    "def get_tickers_from_csv(csv_path: str) -> list:\n",
    "    \"\"\"Load a comprehensive list of tickers from an external CSV file.\"\"\"\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(csv_path)    # Accept common column names for tickers\n",
    "    for col in ['symbol', 'ticker', 'Ticker', 'SYMBOL', 'Symbol']:\n",
    "        if col in df.columns:\n",
    "            tickers = df[col].dropna().unique().tolist()\n",
    "            return tickers\n",
    "    raise ValueError(f\"No ticker column found in {csv_path}. Columns found: {df.columns.tolist()}\")\n",
    "\n",
    "# Example usage:\n",
    "# Download a full US stock list from NASDAQ, NYSE, AMEX, or use a third-party source like 'eodhistoricaldata.com', 'nasdaqtrader.com', or 'stockanalysis.com'.\n",
    "# Place the CSV in your data directory, e.g., 'data/all_us_tickers.csv'.\n",
    "# all_tickers = get_tickers_from_csv('data/all_us_tickers.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d124b8",
   "metadata": {
    "papermill": {
     "duration": 0.003181,
     "end_time": "2025-11-29T05:26:59.715102",
     "exception": false,
     "start_time": "2025-11-29T05:26:59.711921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Break Out Model Components\n",
    "Separate the workflow into modular functions for data loading, preprocessing, model definition, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "485dba8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T05:26:59.724126Z",
     "iopub.status.busy": "2025-11-29T05:26:59.723776Z",
     "iopub.status.idle": "2025-11-29T05:26:59.744458Z",
     "shell.execute_reply": "2025-11-29T05:26:59.743627Z"
    },
    "papermill": {
     "duration": 0.027029,
     "end_time": "2025-11-29T05:26:59.745810",
     "exception": false,
     "start_time": "2025-11-29T05:26:59.718781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data loading function\n",
    "def load_stock_data(csv_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load historical stock data from CSV or other sources.\"\"\"\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "# Quality filter function\n",
    "def filter_quality_stocks(df: pd.DataFrame, min_price=5.0, min_volume=500000) -> pd.DataFrame:\n",
    "    \"\"\"Filter out low-quality stocks (penny stocks, illiquid stocks).\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Minimum price filter (avoid penny stocks)\n",
    "    df = df[df['close'] >= min_price]\n",
    "    \n",
    "    # Minimum volume filter (ensure liquidity)\n",
    "    df = df[df['volume'] >= min_volume]\n",
    "    \n",
    "    # Price stability check (avoid stocks with extreme volatility)\n",
    "    df['price_std_30d'] = df.groupby('symbol')['close'].transform(\n",
    "        lambda x: x.rolling(30, min_periods=1).std()\n",
    "    )\n",
    "    df['volatility'] = df['price_std_30d'] / df['close']\n",
    "    \n",
    "    # Filter out extremely volatile stocks (>50% daily volatility)\n",
    "    df = df[df['volatility'] < 0.5]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Calculate momentum features\n",
    "def calculate_momentum_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add momentum indicators to help identify sustained moves.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Sort by symbol and date\n",
    "    df = df.sort_values(['symbol', 'date'])\n",
    "    \n",
    "    # RSI (Relative Strength Index)\n",
    "    delta = df.groupby('symbol')['close'].diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(14).mean()\n",
    "    loss = -delta.where(delta < 0, 0).rolling(14).mean()\n",
    "    rs = gain / (loss + 1e-10)  # Avoid division by zero\n",
    "    df['rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # MACD\n",
    "    ema_12 = df.groupby('symbol')['close'].transform(lambda x: x.ewm(span=12, adjust=False).mean())\n",
    "    ema_26 = df.groupby('symbol')['close'].transform(lambda x: x.ewm(span=26, adjust=False).mean())\n",
    "    df['macd'] = ema_12 - ema_26\n",
    "    df['macd_signal'] = df.groupby('symbol')['macd'].transform(lambda x: x.ewm(span=9, adjust=False).mean())\n",
    "    df['macd_histogram'] = df['macd'] - df['macd_signal']\n",
    "    \n",
    "    # Rate of Change (10-day)\n",
    "    df['roc_10'] = (df.groupby('symbol')['close'].shift(0) / \n",
    "                    df.groupby('symbol')['close'].shift(10) - 1) * 100\n",
    "    \n",
    "    # Average True Range (volatility measure)\n",
    "    df['prev_close'] = df.groupby('symbol')['close'].shift(1)\n",
    "    df['tr'] = df[['high', 'low', 'prev_close']].apply(\n",
    "        lambda x: max(x['high'] - x['low'], \n",
    "                     abs(x['high'] - x['prev_close']) if pd.notna(x['prev_close']) else 0,\n",
    "                     abs(x['low'] - x['prev_close']) if pd.notna(x['prev_close']) else 0),\n",
    "        axis=1\n",
    "    )\n",
    "    df['atr'] = df.groupby('symbol')['tr'].transform(lambda x: x.rolling(14, min_periods=1).mean())\n",
    "    \n",
    "    # Volume ratio\n",
    "    df['avg_volume_30d'] = df.groupby('symbol')['volume'].transform(\n",
    "        lambda x: x.rolling(30, min_periods=1).mean()\n",
    "    )\n",
    "    df['volume_ratio'] = df['volume'] / (df['avg_volume_30d'] + 1)  # Avoid division by zero\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Preprocessing function with trend filtering and sustained breakout definition\n",
    "def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Feature engineering and breakout labeling with trend filtering.\n",
    "    Labels sustained breakouts (not just one-day spikes).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ensure all price/volume columns are numeric\n",
    "    for col in ['open', 'high', 'low', 'close', 'volume']:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Sort by symbol and date\n",
    "    df = df.sort_values(['symbol', 'date'])\n",
    "    \n",
    "    # Calculate moving averages for trend detection\n",
    "    df['sma_50'] = df.groupby('symbol')['close'].transform(lambda x: x.rolling(50, min_periods=1).mean())\n",
    "    df['sma_200'] = df.groupby('symbol')['close'].transform(lambda x: x.rolling(200, min_periods=1).mean())\n",
    "    \n",
    "    # Trend filter: Price must be above both 50 and 200 day MA\n",
    "    df['uptrend'] = (df['close'] > df['sma_50']) & (df['close'] > df['sma_200'])\n",
    "    \n",
    "    # Filter out downtrending stocks BEFORE labeling breakouts\n",
    "    print(f\"Before trend filter: {len(df)} rows\")\n",
    "    df = df[df['uptrend'] == True].copy()\n",
    "    print(f\"After trend filter (uptrend only): {len(df)} rows\")\n",
    "    \n",
    "    # Calculate multiple forward returns for sustained breakout detection\n",
    "    valid = (df['close'].notnull()) & (df['close'] != 0)\n",
    "    \n",
    "    for days in [30, 60, 90]:\n",
    "        col_name = f'forward_return_{days}d'\n",
    "        df[col_name] = np.nan\n",
    "        shifted = df.groupby('symbol')['close'].shift(-days)\n",
    "        df.loc[valid, col_name] = shifted[valid] / df['close'][valid] - 1\n",
    "    \n",
    "    # Calculate max drawdown during breakout period (30 days)\n",
    "    df['future_min_price'] = df.groupby('symbol')['close'].transform(\n",
    "        lambda x: x.rolling(30, min_periods=1).min().shift(-30)\n",
    "    )\n",
    "    df['max_drawdown_30d'] = (df['future_min_price'] / df['close']) - 1\n",
    "    \n",
    "    # Volume confirmation\n",
    "    df['volume_confirmed'] = df['volume_ratio'] > 1.2\n",
    "    \n",
    "    # Breakout definition: SUSTAINED growth across all periods + volume + no major drawdown\n",
    "    df['breakout'] = (\n",
    "        (df['forward_return_30d'] > 0.20) &   # Up 20%+ at 30 days\n",
    "        (df['forward_return_60d'] > 0.30) &   # Up 30%+ at 60 days\n",
    "        (df['forward_return_90d'] > 0.40) &   # Up 40%+ at 90 days\n",
    "        (df['max_drawdown_30d'] > -0.15) &    # No major crash (>15% drop)\n",
    "        (df['volume_confirmed'] == True)      # Volume above average\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Drop rows with any NaN in required feature columns\n",
    "    df = df.dropna(subset=['open', 'high', 'low', 'close', 'volume', 'rsi', 'macd', 'atr'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # ROC AUC\n",
    "    if len(np.unique(y_test)) > 1:\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        print(f\"ROC AUC: {auc:.3f}\")\n",
    "    else:\n",
    "        print(\"ROC AUC: N/A (only one class in test set)\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['No Breakout', 'Breakout']))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"True Negatives:  {cm[0,0]:>6}\")\n",
    "    print(f\"False Positives: {cm[0,1]:>6}\")\n",
    "    print(f\"False Negatives: {cm[1,0]:>6}\")\n",
    "    print(f\"True Positives:  {cm[1,1]:>6}\")\n",
    "    \n",
    "    # Breakout prediction stats\n",
    "    print(f\"\\nBreakout predictions: {y_pred.sum()} out of {len(y_pred)} ({y_pred.sum()/len(y_pred)*100:.1f}%)\")\n",
    "    print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1ddcb0",
   "metadata": {
    "papermill": {
     "duration": 0.003692,
     "end_time": "2025-11-29T05:26:59.752924",
     "exception": false,
     "start_time": "2025-11-29T05:26:59.749232",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Add Data Point Functionality\n",
    "Implement a function to add or update individual data points for training or testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ae8e99a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T05:26:59.761633Z",
     "iopub.status.busy": "2025-11-29T05:26:59.761107Z",
     "iopub.status.idle": "2025-11-29T05:26:59.766245Z",
     "shell.execute_reply": "2025-11-29T05:26:59.765554Z"
    },
    "papermill": {
     "duration": 0.011154,
     "end_time": "2025-11-29T05:26:59.767650",
     "exception": false,
     "start_time": "2025-11-29T05:26:59.756496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to add or update a data point\n",
    "def add_data_point(df: pd.DataFrame, new_row: Dict) -> pd.DataFrame:\n",
    "    \"\"\"Add or update a single data point in the DataFrame.\"\"\"\n",
    "    df = df.copy()\n",
    "    # Assume 'date' and 'symbol' uniquely identify a row\n",
    "    mask = (df['date'] == new_row['date']) & (df['symbol'] == new_row['symbol'])\n",
    "    if mask.any():\n",
    "        df.loc[mask, :] = pd.DataFrame([new_row])\n",
    "    else:\n",
    "        df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc52457",
   "metadata": {
    "papermill": {
     "duration": 0.003533,
     "end_time": "2025-11-29T05:26:59.774385",
     "exception": false,
     "start_time": "2025-11-29T05:26:59.770852",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Display and Edit Training Cell\n",
    "This cell trains the breakout classifier. You can edit model parameters or code as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "522d0cd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T05:26:59.783712Z",
     "iopub.status.busy": "2025-11-29T05:26:59.783407Z",
     "iopub.status.idle": "2025-11-29T06:59:38.130816Z",
     "shell.execute_reply": "2025-11-29T06:59:38.130134Z"
    },
    "papermill": {
     "duration": 5558.353913,
     "end_time": "2025-11-29T06:59:38.132068",
     "exception": false,
     "start_time": "2025-11-29T05:26:59.778155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING BREAKOUT CLASSIFIER ON 5000+ STOCKS\n",
      "============================================================\n",
      "Loaded 5519 tickers from /kaggle/input/stockss-list/stocks-list.csv\n",
      "Estimated download time: 275.9 minutes\n",
      "Start time: 05:26:59\n",
      "\n",
      "============================================================\n",
      "DOWNLOADING HISTORICAL DATA\n",
      "============================================================\n",
      "\n",
      "Batch 1 (1-100 of 5519) [0.0%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 29452 rows from 100 stocks\n",
      "Progress: 29,452 rows | 1 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 2 (101-200 of 5519) [1.8%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "No data for AEC\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Failed to download AGM.A: Expecting value: line 1 column 1 (char 0)\n",
      "Failed to download AGM.PRH: Expecting value: line 1 column 1 (char 0)\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "No data for AIIA\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Failed to download AKO.A: Expecting value: line 1 column 1 (char 0)\n",
      "Failed to download AKO.B: Expecting value: line 1 column 1 (char 0)\n",
      "Downloaded 26781 rows from 94 stocks\n",
      "Progress: 56,233 rows | 2 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 3 (201-300 of 5519) [3.6%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 29460 rows from 100 stocks\n",
      "Progress: 85,693 rows | 3 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 4 (301-400 of 5519) [5.4%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "No data for APAD\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 28775 rows from 99 stocks\n",
      "Progress: 114,468 rows | 4 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 5 (401-500 of 5519) [7.2%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 29047 rows from 100 stocks\n",
      "Progress: 143,515 rows | 5 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 6 (501-600 of 5519) [9.1%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 28516 rows from 100 stocks\n",
      "Progress: 172,031 rows | 6 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 7 (601-700 of 5519) [10.9%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "No data for BCSS\n",
      "No data for BDCI\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Failed to download BF.A: Expecting value: line 1 column 1 (char 0)\n",
      "Failed to download BF.B: Expecting value: line 1 column 1 (char 0)\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Failed to download BH.A: Expecting value: line 1 column 1 (char 0)\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Failed to download BIO.B: Expecting value: line 1 column 1 (char 0)\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 27770 rows from 94 stocks\n",
      "Progress: 199,801 rows | 7 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 8 (701-800 of 5519) [12.7%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "No data for BLZR\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "No data for BRK.A\n",
      "No data for BRK.B\n",
      "Failed to download BRRW.U: Expecting value: line 1 column 1 (char 0)\n",
      "Downloaded 26801 rows from 96 stocks\n",
      "Progress: 226,602 rows | 8 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 9 (801-900 of 5519) [14.5%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 28766 rows from 100 stocks\n",
      "Progress: 255,368 rows | 9 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 10 (901-1000 of 5519) [16.3%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 28223 rows from 100 stocks\n",
      "Progress: 283,591 rows | 10 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 11 (1001-1100 of 5519) [18.1%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "No data for CHEC\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Failed to download CIG.C: Expecting value: line 1 column 1 (char 0)\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 27939 rows from 98 stocks\n",
      "Progress: 311,530 rows | 11 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 12 (1101-1200 of 5519) [19.9%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 29340 rows from 100 stocks\n",
      "Progress: 340,870 rows | 12 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 13 (1201-1300 of 5519) [21.7%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "No data for CRA\n",
      "Failed to download CRD.A: Expecting value: line 1 column 1 (char 0)\n",
      "Failed to download CRD.B: Expecting value: line 1 column 1 (char 0)\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Failed to download CRNG: Expecting value: line 1 column 1 (char 0)\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 28008 rows from 96 stocks\n",
      "Progress: 368,878 rows | 13 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 14 (1301-1400 of 5519) [23.6%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "No data for CTW\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Failed to download CWEN.A: Expecting value: line 1 column 1 (char 0)\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 28785 rows from 98 stocks\n",
      "Progress: 397,663 rows | 14 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 15 (1401-1500 of 5519) [25.4%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Failed to download DMII: Expecting value: line 1 column 1 (char 0)\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 28844 rows from 99 stocks\n",
      "Progress: 426,507 rows | 15 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 16 (1501-1600 of 5519) [27.2%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 29254 rows from 100 stocks\n",
      "Progress: 455,761 rows | 16 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 17 (1601-1700 of 5519) [29.0%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "No data for ELOG\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 28817 rows from 99 stocks\n",
      "Progress: 484,578 rows | 17 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 18 (1701-1800 of 5519) [30.8%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 28853 rows from 100 stocks\n",
      "Progress: 513,431 rows | 18 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 19 (1801-1900 of 5519) [32.6%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "No data for FCRS\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 28198 rows from 99 stocks\n",
      "Progress: 541,629 rows | 19 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 20 (1901-2000 of 5519) [34.4%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "No data for FOFO\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 29043 rows from 99 stocks\n",
      "Progress: 570,672 rows | 20 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 21 (2001-2100 of 5519) [36.2%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "No data for GEF.B\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 29428 rows from 99 stocks\n",
      "Progress: 600,100 rows | 21 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 22 (2101-2200 of 5519) [38.1%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Failed to download GRP.U: Expecting value: line 1 column 1 (char 0)\n",
      "No data for GSRF\n",
      "Downloaded 28991 rows from 98 stocks\n",
      "Progress: 629,091 rows | 22 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 23 (2201-2300 of 5519) [39.9%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Failed to download GTER: Expecting value: line 1 column 1 (char 0)\n",
      "Failed to download GTER.A: Expecting value: line 1 column 1 (char 0)\n",
      "Failed to download GTN.A: Expecting value: line 1 column 1 (char 0)\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Failed to download HEI.A: Expecting value: line 1 column 1 (char 0)\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 27991 rows from 96 stocks\n",
      "Progress: 657,082 rows | 23 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 24 (2301-2400 of 5519) [41.7%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "No data for HVMC\n",
      "Failed to download HVT.A: Expecting value: line 1 column 1 (char 0)\n",
      "Downloaded 28652 rows from 98 stocks\n",
      "Progress: 685,734 rows | 24 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 25 (2401-2500 of 5519) [43.5%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 29855 rows from 100 stocks\n",
      "Progress: 715,589 rows | 25 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 26 (2501-2600 of 5519) [45.3%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 29021 rows from 100 stocks\n",
      "Progress: 744,610 rows | 26 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 27 (2601-2700 of 5519) [47.1%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 28795 rows from 100 stocks\n",
      "Progress: 773,405 rows | 27 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 28 (2701-2800 of 5519) [48.9%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "No data for KOYN\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "No data for KRSP\n",
      "Downloaded 28146 rows from 98 stocks\n",
      "Progress: 801,551 rows | 28 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 29 (2801-2900 of 5519) [50.7%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "No data for LATA\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "No data for LEN.B\n",
      "No data for LFS\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 27562 rows from 97 stocks\n",
      "Progress: 829,113 rows | 29 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 30 (2901-3000 of 5519) [52.5%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "No data for LKSP\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 29280 rows from 99 stocks\n",
      "Progress: 858,393 rows | 30 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 31 (3001-3100 of 5519) [54.4%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "No data for MBVI\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 28128 rows from 99 stocks\n",
      "Progress: 886,521 rows | 31 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 32 (3101-3200 of 5519) [56.2%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Failed to download MKC.V: Expecting value: line 1 column 1 (char 0)\n",
      "No data for MKLY\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 28341 rows from 98 stocks\n",
      "Progress: 914,862 rows | 32 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 33 (3201-3300 of 5519) [58.0%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Failed to download MOG.A: Expecting value: line 1 column 1 (char 0)\n",
      "Failed to download MOG.B: Expecting value: line 1 column 1 (char 0)\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 28820 rows from 98 stocks\n",
      "Progress: 943,682 rows | 33 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 34 (3301-3400 of 5519) [59.8%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 29209 rows from 100 stocks\n",
      "Progress: 972,891 rows | 34 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 35 (3401-3500 of 5519) [61.6%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "No data for NMP\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 28228 rows from 99 stocks\n",
      "Progress: 1,001,119 rows | 35 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 36 (3501-3600 of 5519) [63.4%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 28706 rows from 100 stocks\n",
      "Progress: 1,029,825 rows | 36 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 37 (3601-3700 of 5519) [65.2%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "No data for ONCH\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "No data for ORIQ\n",
      "Downloaded 28805 rows from 98 stocks\n",
      "Progress: 1,058,630 rows | 37 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 38 (3701-3800 of 5519) [67.0%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "No data for OTGA\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Failed to download PBR.A: Expecting value: line 1 column 1 (char 0)\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 28001 rows from 98 stocks\n",
      "Progress: 1,086,631 rows | 38 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 39 (3801-3900 of 5519) [68.9%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 28131 rows from 100 stocks\n",
      "Progress: 1,114,762 rows | 39 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 40 (3901-4000 of 5519) [70.7%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 29153 rows from 100 stocks\n",
      "Progress: 1,143,915 rows | 40 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 41 (4001-4100 of 5519) [72.5%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "No data for QUMS\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 27980 rows from 99 stocks\n",
      "Progress: 1,171,895 rows | 41 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 42 (4101-4200 of 5519) [74.3%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 29416 rows from 100 stocks\n",
      "Progress: 1,201,311 rows | 42 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 43 (4201-4300 of 5519) [76.1%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "No data for RNGT\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Failed to download RUSH.A: Expecting value: line 1 column 1 (char 0)\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 28364 rows from 98 stocks\n",
      "Progress: 1,229,675 rows | 43 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 44 (4301-4400 of 5519) [77.9%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 29521 rows from 100 stocks\n",
      "Progress: 1,259,196 rows | 44 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 45 (4401-4500 of 5519) [79.7%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 29127 rows from 100 stocks\n",
      "Progress: 1,288,323 rows | 45 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 46 (4501-4600 of 5519) [81.5%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "No data for SOCA\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "No data for SPEG\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 28933 rows from 98 stocks\n",
      "Progress: 1,317,256 rows | 46 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 47 (4601-4700 of 5519) [83.3%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "No data for SSEA\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 28654 rows from 99 stocks\n",
      "Progress: 1,345,910 rows | 47 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 48 (4701-4800 of 5519) [85.2%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Failed to download TAP.A: Expecting value: line 1 column 1 (char 0)\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 28355 rows from 99 stocks\n",
      "Progress: 1,374,265 rows | 48 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 49 (4801-4900 of 5519) [87.0%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "No data for TLNC\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 28487 rows from 99 stocks\n",
      "Progress: 1,402,752 rows | 49 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 50 (4901-5000 of 5519) [88.8%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 29135 rows from 100 stocks\n",
      "Progress: 1,431,887 rows | 50 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 51 (5001-5100 of 5519) [90.6%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Failed to download UHAL.B: Expecting value: line 1 column 1 (char 0)\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 29606 rows from 99 stocks\n",
      "Progress: 1,461,493 rows | 51 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 52 (5101-5200 of 5519) [92.4%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 29004 rows from 100 stocks\n",
      "Progress: 1,490,497 rows | 52 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 53 (5201-5300 of 5519) [94.2%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 28811 rows from 100 stocks\n",
      "Progress: 1,519,308 rows | 53 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 54 (5301-5400 of 5519) [96.0%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Failed to download WSO.B: Expecting value: line 1 column 1 (char 0)\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 28480 rows from 99 stocks\n",
      "Progress: 1,547,788 rows | 54 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 55 (5401-5500 of 5519) [97.8%]\n",
      "Downloading batch 1 (1-20) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 2 (21-40) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 3 (41-60) of 100...\n",
      "No data for YCY\n",
      "No data for YDES\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 4 (61-80) of 100...\n",
      "Sleeping for 1 seconds to avoid API rate limits...\n",
      "Downloading batch 5 (81-100) of 100...\n",
      "Downloaded 28023 rows from 98 stocks\n",
      "Progress: 1,575,811 rows | 55 batches | 0 failures\n",
      "Cooling down for 3 seconds...\n",
      "\n",
      "Batch 56 (5501-5519 of 5519) [99.7%]\n",
      "Downloading batch 1 (1-19) of 19...\n",
      "Downloaded 5572 rows from 19 stocks\n",
      "Progress: 1,581,383 rows | 56 batches | 0 failures\n",
      "\n",
      "============================================================\n",
      "COMBINING DATA\n",
      "============================================================\n",
      "Combined 1,581,383 total rows\n",
      "Unique stocks: 5453\n",
      "Failed tickers: 0\n",
      "Columns in bulk_df: ['date', 'symbol', 'interval', 'open', 'high', 'low', 'close', 'adjusted_close', 'volume']\n",
      "Required columns verified: 'close' and 'date' present\n",
      "\n",
      "============================================================\n",
      "FILTERING QUALITY STOCKS\n",
      "============================================================\n",
      "Before filter: 1,581,383 rows\n",
      "After filter: 471,880 rows\n",
      "Removed: 1,109,503 low-quality data points\n",
      "Remaining stocks: 3703\n",
      "\n",
      "============================================================\n",
      "CALCULATING MOMENTUM FEATURES\n",
      "============================================================\n",
      "Calculated in 14.8 seconds\n",
      "Features added: RSI, MACD, ROC, ATR, Volume Ratio\n",
      "\n",
      "============================================================\n",
      "LABELING SUSTAINED BREAKOUTS\n",
      "============================================================\n",
      "Before trend filter: 471880 rows\n",
      "After trend filter (uptrend only): 225256 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n",
      "  return op(a, b)\n",
      "/tmp/ipykernel_13/3009595576.py:201: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X = bulk_df[features].fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL DATASET:\n",
      "   Total rows: 225,244\n",
      "   Unique stocks: 3341\n",
      "   Date range: 2020-01-06 00:00:00 to 2025-11-28 00:00:00\n",
      "   Breakouts found: 2,077\n",
      "   Breakout rate: 0.92%\n",
      "\n",
      "Sufficient training data: 2077 breakouts\n",
      "\n",
      "============================================================\n",
      "PREPARING TRAINING DATA\n",
      "============================================================\n",
      "Feature matrix shape: (225244, 13)\n",
      "Features: 13\n",
      "Class distribution:\n",
      "   No Breakout: 223,167 (99.1%)\n",
      "   Breakout:    2,077 (0.9%)\n",
      "\n",
      "============================================================\n",
      "SPLITTING TRAIN/TEST DATA\n",
      "============================================================\n",
      "Stratified split successful\n",
      "Train: 180,195 samples (1,662 breakouts, 0.92%)\n",
      "Test:  45,049 samples (415 breakouts, 0.92%)\n",
      "\n",
      "============================================================\n",
      "TRAINING XGBOOST CLASSIFIER\n",
      "============================================================\n",
      "Scale pos weight: 107.42\n",
      "Training complete in 2.0 seconds!\n",
      "\n",
      "============================================================\n",
      "MODEL EVALUATION\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "MODEL EVALUATION\n",
      "============================================================\n",
      "ROC AUC: 0.956\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No Breakout       0.99      1.00      1.00     44634\n",
      "    Breakout       0.55      0.04      0.08       415\n",
      "\n",
      "    accuracy                           0.99     45049\n",
      "   macro avg       0.77      0.52      0.54     45049\n",
      "weighted avg       0.99      0.99      0.99     45049\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "True Negatives:   44619\n",
      "False Positives:     15\n",
      "False Negatives:    397\n",
      "True Positives:      18\n",
      "\n",
      "Breakout predictions: 33 out of 45049 (0.1%)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "TOP 10 MOST IMPORTANT FEATURES\n",
      "============================================================\n",
      "       feature  importance\n",
      "  volume_ratio  0.36787945\n",
      "        sma_50  0.07012392\n",
      "           atr  0.06544860\n",
      "           low  0.06125234\n",
      "       sma_200  0.06026557\n",
      "          open  0.05728150\n",
      "         close  0.05446271\n",
      "          high  0.05156643\n",
      "        volume  0.04827819\n",
      "macd_histogram  0.04306470\n",
      "\n",
      "============================================================\n",
      "SAVING MODEL\n",
      "============================================================\n",
      "Detected Kaggle environment\n",
      "Model saved to: /kaggle/working/breakout_classifier_xgb_stockslist.pkl\n",
      "Metadata saved to: /kaggle/working/breakout_classifier_metadata.json\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE!\n",
      "============================================================\n",
      "Trained on 3341 stocks\n",
      "Found 2,077 sustained breakouts\n",
      "Model finds SUSTAINED breakouts in UPTRENDING stocks\n",
      "Model location: /kaggle/working/breakout_classifier_xgb_stockslist.pkl\n",
      "\n",
      "To download: Go to Output tab on the right\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Training cell: Handle 5000+ stocks with batching and progress tracking\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'start_date': '2020-01-01',  # 5 years of data\n",
    "    'batch_size': 100,  # Process 100 stocks at a time\n",
    "    'api_delay': 3,  # Seconds between API batches\n",
    "    'min_price': 5.0,\n",
    "    'min_volume': 500000,\n",
    "    'max_stocks': None,  # None = use all stocks from CSV\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING BREAKOUT CLASSIFIER ON 5000+ STOCKS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Load tickers from CSV - try multiple paths (Kaggle, Colab, Local)\n",
    "csv_path = None\n",
    "for path in [\n",
    "    # Kaggle paths (when using \"Add Data\" feature)\n",
    "    '/kaggle/input/stockss-list/stocks-list.csv',\n",
    "    '/kaggle/input/stock-list/stocks-list.csv',\n",
    "    '/kaggle/working/stocks-list.csv',\n",
    "    # Colab paths\n",
    "    '/content/stocks-list.csv',\n",
    "    '/stocks-list.csv',\n",
    "    # Current directory\n",
    "    'stocks-list.csv',\n",
    "    # Local development paths\n",
    "    '../data/stocks-list.csv',\n",
    "    'data/stocks-list.csv',\n",
    "]:\n",
    "    if os.path.exists(path):\n",
    "        csv_path = path\n",
    "        break\n",
    "\n",
    "if csv_path is None:\n",
    "    raise FileNotFoundError(\"stocks-list.csv not found! Please upload it first.\")\n",
    "\n",
    "all_tickers = get_tickers_from_csv(csv_path)\n",
    "\n",
    "if CONFIG['max_stocks']:\n",
    "    all_tickers = all_tickers[:CONFIG['max_stocks']]\n",
    "\n",
    "print(f\"Loaded {len(all_tickers)} tickers from {csv_path}\")\n",
    "print(f\"Estimated download time: {(len(all_tickers) * CONFIG['api_delay'] / 60):.1f} minutes\")\n",
    "print(f\"Start time: {time.strftime('%H:%M:%S')}\")\n",
    "\n",
    "# 2. Download historical data in batches with progress tracking\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DOWNLOADING HISTORICAL DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_dataframes = []\n",
    "failed_tickers = []\n",
    "total_rows = 0\n",
    "\n",
    "for batch_num, i in enumerate(range(0, len(all_tickers), CONFIG['batch_size']), 1):\n",
    "    batch_tickers = all_tickers[i:i + CONFIG['batch_size']]\n",
    "    progress = (i / len(all_tickers)) * 100\n",
    "    \n",
    "    print(f\"\\nBatch {batch_num} ({i+1}-{min(i+CONFIG['batch_size'], len(all_tickers))} of {len(all_tickers)}) [{progress:.1f}%]\")\n",
    "    \n",
    "    try:\n",
    "        batch_df = download_eodhd_bulk(\n",
    "            batch_tickers,\n",
    "            start_date=CONFIG['start_date'],\n",
    "            batch_size=20,  # EODHD internal batch\n",
    "            delay=1  # Shorter delay within batch\n",
    "        )\n",
    "        \n",
    "        if not batch_df.empty:\n",
    "            # Ensure lowercase columns\n",
    "            batch_df.columns = [col.lower() for col in batch_df.columns]\n",
    "            all_dataframes.append(batch_df)\n",
    "            total_rows += len(batch_df)\n",
    "            print(f\"Downloaded {len(batch_df)} rows from {batch_df['symbol'].nunique()} stocks\")\n",
    "        else:\n",
    "            print(f\"Batch {batch_num} returned no data\")\n",
    "            failed_tickers.extend(batch_tickers)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Batch {batch_num} failed: {e}\")\n",
    "        failed_tickers.extend(batch_tickers)\n",
    "    \n",
    "    # Progress update\n",
    "    print(f\"Progress: {total_rows:,} rows | {len(all_dataframes)} batches | {len(failed_tickers)} failures\")\n",
    "    \n",
    "    # Rate limiting between batches\n",
    "    if i + CONFIG['batch_size'] < len(all_tickers):\n",
    "        print(f\"Cooling down for {CONFIG['api_delay']} seconds...\")\n",
    "        time.sleep(CONFIG['api_delay'])\n",
    "\n",
    "# 3. Combine all data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMBINING DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if all_dataframes:\n",
    "    bulk_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    print(f\"Combined {len(bulk_df):,} total rows\")\n",
    "    print(f\"Unique stocks: {bulk_df['symbol'].nunique()}\")\n",
    "    print(f\"Failed tickers: {len(failed_tickers)}\")\n",
    "else:\n",
    "    raise ValueError(\"No data downloaded successfully!\")\n",
    "\n",
    "# 4. Check for required columns\n",
    "print(f\"Columns in bulk_df: {bulk_df.columns.tolist()}\")\n",
    "\n",
    "if 'close' not in bulk_df.columns:\n",
    "    raise ValueError(f\"'close' column not found. Columns: {bulk_df.columns.tolist()}\")\n",
    "\n",
    "# Ensure 'date' column exists (EODHD may return date as index)\n",
    "if 'date' not in bulk_df.columns:\n",
    "    print(\"'date' column not found, attempting to recover from index...\")\n",
    "    bulk_df = bulk_df.reset_index()\n",
    "    # Try common column name variations\n",
    "    for col in bulk_df.columns:\n",
    "        if 'date' in col.lower() or 'time' in col.lower():\n",
    "            bulk_df = bulk_df.rename(columns={col: 'date'})\n",
    "            print(f\"   Renamed '{col}' to 'date'\")\n",
    "            break\n",
    "    # If still no date column, use the first column if it looks like dates\n",
    "    if 'date' not in bulk_df.columns and 'index' in bulk_df.columns:\n",
    "        bulk_df = bulk_df.rename(columns={'index': 'date'})\n",
    "        print(\"   Renamed 'index' to 'date'\")\n",
    "\n",
    "    if 'date' not in bulk_df.columns:\n",
    "        raise ValueError(f\"'date' column not found and could not be recovered. Columns: {bulk_df.columns.tolist()}\")\n",
    "\n",
    "print(f\"Required columns verified: 'close' and 'date' present\")\n",
    "\n",
    "# 5. Filter quality stocks (parallel processing for speed)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FILTERING QUALITY STOCKS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Before filter: {len(bulk_df):,} rows\")\n",
    "\n",
    "bulk_df = filter_quality_stocks(\n",
    "    bulk_df,\n",
    "    min_price=CONFIG['min_price'],\n",
    "    min_volume=CONFIG['min_volume']\n",
    ")\n",
    "\n",
    "print(f\"After filter: {len(bulk_df):,} rows\")\n",
    "print(f\"Removed: {total_rows - len(bulk_df):,} low-quality data points\")\n",
    "print(f\"Remaining stocks: {bulk_df['symbol'].nunique()}\")\n",
    "\n",
    "# 6. Add momentum features (vectorized for speed)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CALCULATING MOMENTUM FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "bulk_df = calculate_momentum_features(bulk_df)\n",
    "calc_time = time.time() - start_time\n",
    "\n",
    "print(f\"Calculated in {calc_time:.1f} seconds\")\n",
    "print(f\"Features added: RSI, MACD, ROC, ATR, Volume Ratio\")\n",
    "\n",
    "# 7. Preprocess with trend filter and sustained breakout definition\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LABELING SUSTAINED BREAKOUTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "bulk_df = preprocess_data(bulk_df)\n",
    "\n",
    "print(f\"\\nFINAL DATASET:\")\n",
    "print(f\"   Total rows: {len(bulk_df):,}\")\n",
    "print(f\"   Unique stocks: {bulk_df['symbol'].nunique()}\")\n",
    "print(f\"   Date range: {bulk_df['date'].min()} to {bulk_df['date'].max()}\")\n",
    "print(f\"   Breakouts found: {bulk_df['breakout'].sum():,}\")\n",
    "print(f\"   Breakout rate: {bulk_df['breakout'].mean():.2%}\")\n",
    "\n",
    "# 8. Validate sufficient training data\n",
    "if bulk_df['breakout'].sum() < 50:\n",
    "    print(\"\\nWARNING: Very few breakouts found (<50). Consider:\")\n",
    "    print(\"   - Lowering return thresholds in preprocess_data()\")\n",
    "    print(\"   - Using more historical data (start_date='2015-01-01')\")\n",
    "    print(\"   - Checking data quality\")\n",
    "elif bulk_df['breakout'].sum() < 200:\n",
    "    print(\"\\nCAUTION: Limited breakouts found (<200). Model may be less robust.\")\n",
    "else:\n",
    "    print(f\"\\nSufficient training data: {bulk_df['breakout'].sum()} breakouts\")\n",
    "\n",
    "# 9. Select features and prepare training data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREPARING TRAINING DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "features = [\n",
    "    'open', 'high', 'low', 'close', 'volume',\n",
    "    'sma_50', 'sma_200', 'rsi', 'macd', 'macd_histogram',\n",
    "    'roc_10', 'atr', 'volume_ratio'\n",
    "]\n",
    "\n",
    "X = bulk_df[features].fillna(0)\n",
    "y = bulk_df['breakout']\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Features: {len(features)}\")\n",
    "print(f\"Class distribution:\")\n",
    "print(f\"   No Breakout: {(y==0).sum():,} ({(y==0).mean():.1%})\")\n",
    "print(f\"   Breakout:    {(y==1).sum():,} ({(y==1).mean():.1%})\")\n",
    "\n",
    "# 10. Train/test split with stratification\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SPLITTING TRAIN/TEST DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    print(\"Stratified split successful\")\n",
    "except ValueError as e:\n",
    "    print(f\"Cannot stratify: {e}\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "print(f\"Train: {len(X_train):,} samples ({y_train.sum():,} breakouts, {y_train.mean():.2%})\")\n",
    "print(f\"Test:  {len(X_test):,} samples ({y_test.sum():,} breakouts, {y_test.mean():.2%})\")\n",
    "\n",
    "# 11. Train model with optimized hyperparameters\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING XGBOOST CLASSIFIER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate class weight to handle imbalance\n",
    "pos_count = y_train.sum()\n",
    "neg_count = len(y_train) - pos_count\n",
    "scale_pos_weight = neg_count / pos_count if pos_count > 0 else 1\n",
    "\n",
    "print(f\"Scale pos weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "clf = BreakoutStockClassifier()\n",
    "clf.model = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    n_jobs=-1,  # Use all CPU cores\n",
    "    tree_method='hist'  # Faster for large datasets\n",
    ")\n",
    "\n",
    "train_start = time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "train_time = time.time() - train_start\n",
    "\n",
    "print(f\"Training complete in {train_time:.1f} seconds!\")\n",
    "\n",
    "# 12. Evaluate model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "evaluate_model(clf.model, X_test, y_test)\n",
    "\n",
    "# 13. Feature importance\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOP 10 MOST IMPORTANT FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': clf.model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(feature_importance.head(10).to_string(index=False))\n",
    "\n",
    "# 14. Save model with metadata - detect environment\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Detect environment and set output directory\n",
    "if os.path.exists('/kaggle/working'):\n",
    "    # Kaggle environment - save to working directory (shows in Output tab)\n",
    "    model_dir = '/kaggle/working'\n",
    "    print(\"Detected Kaggle environment\")\n",
    "elif os.path.exists('/content'):\n",
    "    # Colab environment\n",
    "    model_dir = '/content/ml_models'\n",
    "    print(\"Detected Colab environment\")\n",
    "else:\n",
    "    # Local environment\n",
    "    model_dir = '../python/ml_models'\n",
    "    print(\"Detected local environment\")\n",
    "\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save model with descriptive name\n",
    "model_filename = 'breakout_classifier_xgb_stockslist.pkl'\n",
    "model_path = f'{model_dir}/{model_filename}'\n",
    "clf.save(model_path)\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'training_date': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'total_stocks': len(all_tickers),\n",
    "    'successful_stocks': bulk_df['symbol'].nunique(),\n",
    "    'failed_stocks': len(failed_tickers),\n",
    "    'training_samples': len(X_train),\n",
    "    'test_samples': len(X_test),\n",
    "    'breakout_rate': float(y.mean()),\n",
    "    'features': features,\n",
    "    'config': CONFIG,\n",
    "    'model_performance': {\n",
    "        'training_time_seconds': train_time,\n",
    "        'test_breakouts': int(y_test.sum())\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "metadata_path = f'{model_dir}/breakout_classifier_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "print(f\"Metadata saved to: {metadata_path}\")\n",
    "\n",
    "# 15. Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Trained on {bulk_df['symbol'].nunique()} stocks\")\n",
    "print(f\"Found {bulk_df['breakout'].sum():,} sustained breakouts\")\n",
    "print(f\"Model finds SUSTAINED breakouts in UPTRENDING stocks\")\n",
    "print(f\"Model location: {model_path}\")\n",
    "\n",
    "if '/kaggle/' in model_dir:\n",
    "    print(\"\\nTo download: Go to Output tab on the right\")\n",
    "elif '/content/' in model_dir:\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"   1. Run the download cell below to get a zip file\")\n",
    "    print(\"   2. Upload to your python/ml_models/ directory\")\n",
    "    print(\"   3. Use in your ML API for predictions\")\n",
    "else:\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"   1. Model is saved locally\")\n",
    "    print(\"   2. Use in your ML API for predictions\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72494ef3",
   "metadata": {
    "papermill": {
     "duration": 0.035839,
     "end_time": "2025-11-29T06:59:38.204021",
     "exception": false,
     "start_time": "2025-11-29T06:59:38.168182",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Improved Breakout Detection\n",
    "\n",
    "This updated notebook now finds **real, sustained breakouts** instead of one-day spikes:\n",
    "\n",
    "### Key Improvements:\n",
    "\n",
    "1. **Trend Filter**: Only considers stocks in uptrends (price above 50 & 200-day MA)\n",
    "2. **Sustained Returns**: Requires positive returns at 30, 60, AND 90 days:\n",
    "   - 20%+ at 30 days\n",
    "   - 30%+ at 60 days  \n",
    "   - 40%+ at 90 days\n",
    "3. **Volume Confirmation**: Breakouts need above-average volume (1.2x+)\n",
    "4. **Quality Filter**: Excludes penny stocks (<$5) and illiquid stocks (<500K volume)\n",
    "5. **Drawdown Protection**: Excludes stocks that crash >15% mid-period\n",
    "6. **Momentum Features**: RSI, MACD, ROC help identify real momentum\n",
    "\n",
    "### Expected Results:\n",
    "- Fewer breakout labels (more selective)\n",
    "- Higher quality picks that sustain growth\n",
    "- Better for real trading/investment decisions\n",
    "\n",
    "### If you get too few breakouts:\n",
    "- Lower the return thresholds (e.g., 15%/25%/35%)\n",
    "- Increase the stock universe (use more tickers)\n",
    "- Use more historical data (start_date=\"2015-01-01\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e98214",
   "metadata": {
    "papermill": {
     "duration": 0.035729,
     "end_time": "2025-11-29T06:59:38.275851",
     "exception": false,
     "start_time": "2025-11-29T06:59:38.240122",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Download Models (For Colab Users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3728462e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T06:59:38.348939Z",
     "iopub.status.busy": "2025-11-29T06:59:38.348585Z",
     "iopub.status.idle": "2025-11-29T06:59:38.352854Z",
     "shell.execute_reply": "2025-11-29T06:59:38.351876Z"
    },
    "papermill": {
     "duration": 0.042419,
     "end_time": "2025-11-29T06:59:38.354068",
     "exception": false,
     "start_time": "2025-11-29T06:59:38.311649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download models for Colab (UNCOMMENT if running in Colab)\n",
    "# from google.colab import files\n",
    "# import zipfile\n",
    "# import os\n",
    "\n",
    "# print(\"üì¶ Creating zip file with breakout classifier models...\")\n",
    "\n",
    "# # Create zip file\n",
    "# zip_filename = 'breakout_classifier_models.zip'\n",
    "# with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "#     for file in ['breakout_classifier_xgb_stockslist.pkl', 'breakout_classifier_metadata.json']:\n",
    "#         file_path = f\"{model_dir}/{file}\"\n",
    "#         if os.path.exists(file_path):\n",
    "#             zipf.write(file_path, file)\n",
    "#             print(f\"  ‚úÖ Added {file}\")\n",
    "#         else:\n",
    "#             print(f\"  ‚ö†Ô∏è  {file} not found\")\n",
    "\n",
    "# print(f\"\\n‚¨áÔ∏è  Downloading {zip_filename}...\")\n",
    "# files.download(zip_filename)\n",
    "# print(\"‚úÖ Download complete!\")\n",
    "# print(\"\\nüìã Next steps:\")\n",
    "# print(\"1. Extract the zip file\")\n",
    "# print(\"2. Upload the .pkl file to your project's python/ml_models/ directory\")\n",
    "# print(\"3. Use the model in your ML API for breakout predictions\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8865943,
     "sourceId": 13914278,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5578.320287,
   "end_time": "2025-11-29T06:59:39.209589",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-29T05:26:40.889302",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
