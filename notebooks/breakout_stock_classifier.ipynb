{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca0b1067",
   "metadata": {},
   "source": [
    "# Breakout Stock Classifier: Scaffolding and Expansion\n",
    "\n",
    "This notebook scaffolds a modular backend for a breakout stock classifier, breaks out model components, adds data point functionality, displays and edits the training cell, and updates the workflow to handle more stocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018o3w7gjnev",
   "metadata": {},
   "source": [
    "## 0. Setup & Installation\n",
    "\n",
    "Run this cell first to install required packages (required for Kaggle/Colab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u86h6ddqune",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (UNCOMMENT and run in Kaggle/Colab)\n",
    "!pip install eodhd xgboost scikit-learn pandas numpy joblib -q\n",
    "\n",
    "# Optional: Upload your stock list CSV in Kaggle/Colab\n",
    "# For Kaggle: Use \"Add Data\" button on the right panel instead\n",
    "# For Colab:\n",
    "# from google.colab import files\n",
    "# print(\"üìÅ Upload your stocks-list.csv file:\")\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27461137",
   "metadata": {},
   "source": [
    "## Google Colab / Kaggle: Uploading Your CSV\n",
    "\n",
    "### For Google Colab:\n",
    "\n",
    "If you are using Google Colab, you can upload your `stocks-list.csv` file directly to the Colab runtime with the following code cell:\n",
    "\n",
    "```python\n",
    "from google.colab import files\n",
    "uploaded = files.upload()  # This will prompt you to select and upload your CSV file\n",
    "```\n",
    "\n",
    "- After uploading, the file will be in the current working directory.\n",
    "- If your code expects the file in a `data/` folder, move it with:\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.replace('stocks-list.csv', 'data/stocks-list.csv')\n",
    "```\n",
    "\n",
    "Alternatively, you can mount your Google Drive and access files from there:\n",
    "\n",
    "```python\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# Then use the path: '/content/drive/My Drive/path/to/stocks-list.csv'\n",
    "```\n",
    "\n",
    "### For Kaggle:\n",
    "\n",
    "1. **Upload your dataset to Kaggle:**\n",
    "   - Go to [kaggle.com/datasets](https://www.kaggle.com/datasets)\n",
    "   - Click \"New Dataset\" and upload `stocks-list.csv`\n",
    "   - Name it \"stocks-list\" (or similar)\n",
    "\n",
    "2. **Add the dataset to your notebook:**\n",
    "   - In your Kaggle notebook, click \"Add Data\" on the right panel\n",
    "   - Search for your uploaded dataset\n",
    "   - Add it to your notebook\n",
    "\n",
    "3. **The notebook will automatically detect Kaggle paths:**\n",
    "   - `/kaggle/input/stocks-list/stocks-list.csv`\n",
    "   - `/kaggle/input/stock-list/stocks-list.csv`\n",
    "\n",
    "4. **Output files** will be saved to `/kaggle/working/` and appear in the \"Output\" tab\n",
    "\n",
    "Adjust your code to use the correct path depending on your upload method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca445b9",
   "metadata": {},
   "source": [
    "## 1. Scaffold Model Backend\n",
    "Set up the basic backend structure for the breakout classifier, including imports and class/function definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9db6896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and backend scaffolding\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "\n",
    "# Placeholder for backend class\n",
    "class BreakoutStockClassifier:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.features = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.model = XGBClassifier(n_estimators=200, max_depth=5, random_state=42)\n",
    "        self.model.fit(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict_proba(X)[:, 1] if self.model else None\n",
    "    \n",
    "    def save(self, path):\n",
    "        joblib.dump(self.model, path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        self.model = joblib.load(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c6826e",
   "metadata": {},
   "outputs": [],
   "source": "# --- EOD Historical Data: Download US stock price data (replace Alpha Vantage) ---\n\nfrom eodhd import APIClient\nimport pandas as pd\nimport time\nimport numpy as np\n\n# Use your actual EODHD API token here\nEODHD_API_KEY = \"68ebce6775f004.44089353\"\n\ndef download_eodhd_bulk(tickers, api_key=EODHD_API_KEY, start_date=\"2015-01-01\", end_date=None, batch_size=5, delay=2):\n    \"\"\"\n    Download daily historical data for a list of tickers from EODHD.\n    Returns a DataFrame with all data concatenated.\n    Ensures 'close' and 'date' columns exist for all tickers.\n    \"\"\"\n    # Defensive validation: ensure tickers is a list of strings\n    if tickers is None:\n        print(\"Warning: tickers is None, returning empty DataFrame\")\n        return pd.DataFrame()\n    \n    if not hasattr(tickers, '__iter__') or isinstance(tickers, (str, int, float)):\n        print(f\"Warning: tickers is not iterable (got {type(tickers).__name__}), returning empty DataFrame\")\n        return pd.DataFrame()\n    \n    # Convert to list and filter out non-string items\n    tickers = [t for t in tickers if isinstance(t, str) and t.strip()]\n    \n    if len(tickers) == 0:\n        print(\"Warning: No valid tickers provided, returning empty DataFrame\")\n        return pd.DataFrame()\n    \n    client = APIClient(api_key)\n    all_data = []\n    total = len(tickers)\n    for i in range(0, total, batch_size):\n        batch = tickers[i:i+batch_size]\n        print(f\"Downloading batch {i//batch_size+1} ({i+1}-{min(i+batch_size, total)}) of {total}...\")\n        for ticker in batch:\n            try:\n                df = client.get_historical_data(\n                    symbol=ticker,\n                    interval=\"d\",\n                    iso8601_start=start_date,\n                    iso8601_end=end_date if end_date else \"\"\n                )\n                if not df.empty:\n                    # Ensure 'date' column exists (EODHD may return date as index)\n                    if 'date' not in df.columns:\n                        if df.index.name == 'date' or df.index.name is None:\n                            df = df.reset_index()\n                            # Rename index column to 'date' if needed\n                            if 'index' in df.columns:\n                                df = df.rename(columns={'index': 'date'})\n                            elif df.columns[0] not in ['date', 'open', 'high', 'low', 'close', 'volume']:\n                                df = df.rename(columns={df.columns[0]: 'date'})\n                    \n                    # Ensure 'close' column exists\n                    if 'close' not in df.columns:\n                        print(f\"Ticker {ticker} missing 'close' column, adding as NaN.\")\n                        df['close'] = np.nan\n                    \n                    df['symbol'] = ticker\n                    all_data.append(df)\n                else:\n                    print(f\"No data for {ticker}\")\n            except Exception as e:\n                print(f\"Failed to download {ticker}: {e}\")\n        if i + batch_size < total:\n            print(f\"Sleeping for {delay} seconds to avoid API rate limits...\")\n            time.sleep(delay)\n    if all_data:\n        result_df = pd.concat(all_data, ignore_index=True)\n        # Final check: ensure 'date' column exists\n        if 'date' not in result_df.columns:\n            print(\"Warning: 'date' column still missing after concat, attempting to fix...\")\n            result_df = result_df.reset_index()\n            if 'index' in result_df.columns:\n                result_df = result_df.rename(columns={'index': 'date'})\n        return result_df\n    else:\n        print(\"No valid data downloaded.\")\n        return pd.DataFrame()\n\n# Example usage:\n# us_tickers = [\"AAPL\", \"MSFT\", \"GOOG\"]\n# df = download_eodhd_bulk(us_tickers)\n# print(df.head())\n\n# Remove Alpha Vantage import and function\ndef download_alpha_vantage_bulk(*args, **kwargs):\n    raise NotImplementedError(\"Alpha Vantage integration has been replaced by EODHD.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95b26c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Download and use a comprehensive ticker list from an external CSV ---\n",
    "import os\n",
    "\n",
    "def get_tickers_from_csv(csv_path: str) -> list:\n",
    "    \"\"\"Load a comprehensive list of tickers from an external CSV file.\"\"\"\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(csv_path)    # Accept common column names for tickers\n",
    "    for col in ['symbol', 'ticker', 'Ticker', 'SYMBOL', 'Symbol']:\n",
    "        if col in df.columns:\n",
    "            tickers = df[col].dropna().unique().tolist()\n",
    "            return tickers\n",
    "    raise ValueError(f\"No ticker column found in {csv_path}. Columns found: {df.columns.tolist()}\")\n",
    "\n",
    "# Example usage:\n",
    "# Download a full US stock list from NASDAQ, NYSE, AMEX, or use a third-party source like 'eodhistoricaldata.com', 'nasdaqtrader.com', or 'stockanalysis.com'.\n",
    "# Place the CSV in your data directory, e.g., 'data/all_us_tickers.csv'.\n",
    "# all_tickers = get_tickers_from_csv('data/all_us_tickers.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb7440c",
   "metadata": {},
   "source": [
    "## 2. Break Out Model Components\n",
    "Separate the workflow into modular functions for data loading, preprocessing, model definition, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d65af7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading function\n",
    "def load_stock_data(csv_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load historical stock data from CSV or other sources.\"\"\"\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "# Quality filter function\n",
    "def filter_quality_stocks(df: pd.DataFrame, min_price=5.0, min_volume=500000) -> pd.DataFrame:\n",
    "    \"\"\"Filter out low-quality stocks (penny stocks, illiquid stocks).\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Minimum price filter (avoid penny stocks)\n",
    "    df = df[df['close'] >= min_price]\n",
    "    \n",
    "    # Minimum volume filter (ensure liquidity)\n",
    "    df = df[df['volume'] >= min_volume]\n",
    "    \n",
    "    # Price stability check (avoid stocks with extreme volatility)\n",
    "    df['price_std_30d'] = df.groupby('symbol')['close'].transform(\n",
    "        lambda x: x.rolling(30, min_periods=1).std()\n",
    "    )\n",
    "    df['volatility'] = df['price_std_30d'] / df['close']\n",
    "    \n",
    "    # Filter out extremely volatile stocks (>50% daily volatility)\n",
    "    df = df[df['volatility'] < 0.5]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Calculate momentum features\n",
    "def calculate_momentum_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add momentum indicators to help identify sustained moves.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Sort by symbol and date\n",
    "    df = df.sort_values(['symbol', 'date'])\n",
    "    \n",
    "    # RSI (Relative Strength Index)\n",
    "    delta = df.groupby('symbol')['close'].diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(14).mean()\n",
    "    loss = -delta.where(delta < 0, 0).rolling(14).mean()\n",
    "    rs = gain / (loss + 1e-10)  # Avoid division by zero\n",
    "    df['rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # MACD\n",
    "    ema_12 = df.groupby('symbol')['close'].transform(lambda x: x.ewm(span=12, adjust=False).mean())\n",
    "    ema_26 = df.groupby('symbol')['close'].transform(lambda x: x.ewm(span=26, adjust=False).mean())\n",
    "    df['macd'] = ema_12 - ema_26\n",
    "    df['macd_signal'] = df.groupby('symbol')['macd'].transform(lambda x: x.ewm(span=9, adjust=False).mean())\n",
    "    df['macd_histogram'] = df['macd'] - df['macd_signal']\n",
    "    \n",
    "    # Rate of Change (10-day)\n",
    "    df['roc_10'] = (df.groupby('symbol')['close'].shift(0) / \n",
    "                    df.groupby('symbol')['close'].shift(10) - 1) * 100\n",
    "    \n",
    "    # Average True Range (volatility measure)\n",
    "    df['prev_close'] = df.groupby('symbol')['close'].shift(1)\n",
    "    df['tr'] = df[['high', 'low', 'prev_close']].apply(\n",
    "        lambda x: max(x['high'] - x['low'], \n",
    "                     abs(x['high'] - x['prev_close']) if pd.notna(x['prev_close']) else 0,\n",
    "                     abs(x['low'] - x['prev_close']) if pd.notna(x['prev_close']) else 0),\n",
    "        axis=1\n",
    "    )\n",
    "    df['atr'] = df.groupby('symbol')['tr'].transform(lambda x: x.rolling(14, min_periods=1).mean())\n",
    "    \n",
    "    # Volume ratio\n",
    "    df['avg_volume_30d'] = df.groupby('symbol')['volume'].transform(\n",
    "        lambda x: x.rolling(30, min_periods=1).mean()\n",
    "    )\n",
    "    df['volume_ratio'] = df['volume'] / (df['avg_volume_30d'] + 1)  # Avoid division by zero\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Preprocessing function with trend filtering and sustained breakout definition\n",
    "def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Feature engineering and breakout labeling with trend filtering.\n",
    "    Labels sustained breakouts (not just one-day spikes).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ensure all price/volume columns are numeric\n",
    "    for col in ['open', 'high', 'low', 'close', 'volume']:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Sort by symbol and date\n",
    "    df = df.sort_values(['symbol', 'date'])\n",
    "    \n",
    "    # Calculate moving averages for trend detection\n",
    "    df['sma_50'] = df.groupby('symbol')['close'].transform(lambda x: x.rolling(50, min_periods=1).mean())\n",
    "    df['sma_200'] = df.groupby('symbol')['close'].transform(lambda x: x.rolling(200, min_periods=1).mean())\n",
    "    \n",
    "    # Trend filter: Price must be above both 50 and 200 day MA\n",
    "    df['uptrend'] = (df['close'] > df['sma_50']) & (df['close'] > df['sma_200'])\n",
    "    \n",
    "    # Filter out downtrending stocks BEFORE labeling breakouts\n",
    "    print(f\"Before trend filter: {len(df)} rows\")\n",
    "    df = df[df['uptrend'] == True].copy()\n",
    "    print(f\"After trend filter (uptrend only): {len(df)} rows\")\n",
    "    \n",
    "    # Calculate multiple forward returns for sustained breakout detection\n",
    "    valid = (df['close'].notnull()) & (df['close'] != 0)\n",
    "    \n",
    "    for days in [30, 60, 90]:\n",
    "        col_name = f'forward_return_{days}d'\n",
    "        df[col_name] = np.nan\n",
    "        shifted = df.groupby('symbol')['close'].shift(-days)\n",
    "        df.loc[valid, col_name] = shifted[valid] / df['close'][valid] - 1\n",
    "    \n",
    "    # Calculate max drawdown during breakout period (30 days)\n",
    "    df['future_min_price'] = df.groupby('symbol')['close'].transform(\n",
    "        lambda x: x.rolling(30, min_periods=1).min().shift(-30)\n",
    "    )\n",
    "    df['max_drawdown_30d'] = (df['future_min_price'] / df['close']) - 1\n",
    "    \n",
    "    # Volume confirmation\n",
    "    df['volume_confirmed'] = df['volume_ratio'] > 1.2\n",
    "    \n",
    "    # Breakout definition: SUSTAINED growth across all periods + volume + no major drawdown\n",
    "    df['breakout'] = (\n",
    "        (df['forward_return_30d'] > 0.20) &   # Up 20%+ at 30 days\n",
    "        (df['forward_return_60d'] > 0.30) &   # Up 30%+ at 60 days\n",
    "        (df['forward_return_90d'] > 0.40) &   # Up 40%+ at 90 days\n",
    "        (df['max_drawdown_30d'] > -0.15) &    # No major crash (>15% drop)\n",
    "        (df['volume_confirmed'] == True)      # Volume above average\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Drop rows with any NaN in required feature columns\n",
    "    df = df.dropna(subset=['open', 'high', 'low', 'close', 'volume', 'rsi', 'macd', 'atr'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # ROC AUC\n",
    "    if len(np.unique(y_test)) > 1:\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        print(f\"ROC AUC: {auc:.3f}\")\n",
    "    else:\n",
    "        print(\"ROC AUC: N/A (only one class in test set)\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['No Breakout', 'Breakout']))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"True Negatives:  {cm[0,0]:>6}\")\n",
    "    print(f\"False Positives: {cm[0,1]:>6}\")\n",
    "    print(f\"False Negatives: {cm[1,0]:>6}\")\n",
    "    print(f\"True Positives:  {cm[1,1]:>6}\")\n",
    "    \n",
    "    # Breakout prediction stats\n",
    "    print(f\"\\nBreakout predictions: {y_pred.sum()} out of {len(y_pred)} ({y_pred.sum()/len(y_pred)*100:.1f}%)\")\n",
    "    print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d4bdf9",
   "metadata": {},
   "source": [
    "## 3. Add Data Point Functionality\n",
    "Implement a function to add or update individual data points for training or testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4819ba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add or update a data point\n",
    "def add_data_point(df: pd.DataFrame, new_row: Dict) -> pd.DataFrame:\n",
    "    \"\"\"Add or update a single data point in the DataFrame.\"\"\"\n",
    "    df = df.copy()\n",
    "    # Assume 'date' and 'symbol' uniquely identify a row\n",
    "    mask = (df['date'] == new_row['date']) & (df['symbol'] == new_row['symbol'])\n",
    "    if mask.any():\n",
    "        df.loc[mask, :] = pd.DataFrame([new_row])\n",
    "    else:\n",
    "        df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e76693c",
   "metadata": {},
   "source": [
    "## 4. Display and Edit Training Cell\n",
    "This cell trains the breakout classifier. You can edit model parameters or code as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675009f9",
   "metadata": {},
   "outputs": [],
   "source": "# Training cell: Handle 5000+ stocks with batching and progress tracking\n\nfrom pathlib import Path\nimport time\nimport os\n\n# Configuration\nCONFIG = {\n    'start_date': '2020-01-01',  # 5 years of data\n    'batch_size': 100,  # Process 100 stocks at a time\n    'api_delay': 3,  # Seconds between API batches\n    'min_price': 5.0,\n    'min_volume': 500000,\n    'max_stocks': None,  # None = use all stocks from CSV\n}\n\nprint(\"=\"*60)\nprint(\"TRAINING BREAKOUT CLASSIFIER ON 5000+ STOCKS\")\nprint(\"=\"*60)\n\n# 1. Load tickers from CSV - try multiple paths (Kaggle, Colab, Local)\ncsv_path = None\nfor path in [\n    # Kaggle paths (when using \"Add Data\" feature)\n    '/kaggle/input/stocks-list/stocks-list.csv',\n    '/kaggle/input/stock-list/stocks-list.csv',\n    '/kaggle/working/stocks-list.csv',\n    # Colab paths\n    '/content/stocks-list.csv',\n    '/stocks-list.csv',\n    # Current directory\n    'stocks-list.csv',\n    # Local development paths\n    '../data/stocks-list.csv',\n    'data/stocks-list.csv',\n]:\n    if os.path.exists(path):\n        csv_path = path\n        break\n\nif csv_path is None:\n    raise FileNotFoundError(\"stocks-list.csv not found! Please upload it first.\")\n\nall_tickers = get_tickers_from_csv(csv_path)\n\nif CONFIG['max_stocks']:\n    all_tickers = all_tickers[:CONFIG['max_stocks']]\n\nprint(f\"Loaded {len(all_tickers)} tickers from {csv_path}\")\nprint(f\"Estimated download time: {(len(all_tickers) * CONFIG['api_delay'] / 60):.1f} minutes\")\nprint(f\"Start time: {time.strftime('%H:%M:%S')}\")\n\n# 2. Download historical data in batches with progress tracking\nprint(\"\\n\" + \"=\"*60)\nprint(\"DOWNLOADING HISTORICAL DATA\")\nprint(\"=\"*60)\n\nall_dataframes = []\nfailed_tickers = []\ntotal_rows = 0\n\nfor batch_num, i in enumerate(range(0, len(all_tickers), CONFIG['batch_size']), 1):\n    batch_tickers = all_tickers[i:i + CONFIG['batch_size']]\n    progress = (i / len(all_tickers)) * 100\n    \n    print(f\"\\nBatch {batch_num} ({i+1}-{min(i+CONFIG['batch_size'], len(all_tickers))} of {len(all_tickers)}) [{progress:.1f}%]\")\n    \n    try:\n        batch_df = download_eodhd_bulk(\n            batch_tickers,\n            start_date=CONFIG['start_date'],\n            batch_size=20,  # EODHD internal batch\n            delay=1  # Shorter delay within batch\n        )\n        \n        if not batch_df.empty:\n            # Ensure lowercase columns\n            batch_df.columns = [col.lower() for col in batch_df.columns]\n            all_dataframes.append(batch_df)\n            total_rows += len(batch_df)\n            print(f\"Downloaded {len(batch_df)} rows from {batch_df['symbol'].nunique()} stocks\")\n        else:\n            print(f\"Batch {batch_num} returned no data\")\n            failed_tickers.extend(batch_tickers)\n    \n    except Exception as e:\n        print(f\"Batch {batch_num} failed: {e}\")\n        failed_tickers.extend(batch_tickers)\n    \n    # Progress update\n    print(f\"Progress: {total_rows:,} rows | {len(all_dataframes)} batches | {len(failed_tickers)} failures\")\n    \n    # Rate limiting between batches\n    if i + CONFIG['batch_size'] < len(all_tickers):\n        print(f\"Cooling down for {CONFIG['api_delay']} seconds...\")\n        time.sleep(CONFIG['api_delay'])\n\n# 3. Combine all data\nprint(\"\\n\" + \"=\"*60)\nprint(\"COMBINING DATA\")\nprint(\"=\"*60)\n\nif all_dataframes:\n    bulk_df = pd.concat(all_dataframes, ignore_index=True)\n    print(f\"Combined {len(bulk_df):,} total rows\")\n    print(f\"Unique stocks: {bulk_df['symbol'].nunique()}\")\n    print(f\"Failed tickers: {len(failed_tickers)}\")\nelse:\n    raise ValueError(\"No data downloaded successfully!\")\n\n# 4. Check for required columns\nprint(f\"Columns in bulk_df: {bulk_df.columns.tolist()}\")\n\nif 'close' not in bulk_df.columns:\n    raise ValueError(f\"'close' column not found. Columns: {bulk_df.columns.tolist()}\")\n\n# Ensure 'date' column exists (EODHD may return date as index)\nif 'date' not in bulk_df.columns:\n    print(\"'date' column not found, attempting to recover from index...\")\n    bulk_df = bulk_df.reset_index()\n    # Try common column name variations\n    for col in bulk_df.columns:\n        if 'date' in col.lower() or 'time' in col.lower():\n            bulk_df = bulk_df.rename(columns={col: 'date'})\n            print(f\"   Renamed '{col}' to 'date'\")\n            break\n    # If still no date column, use the first column if it looks like dates\n    if 'date' not in bulk_df.columns and 'index' in bulk_df.columns:\n        bulk_df = bulk_df.rename(columns={'index': 'date'})\n        print(\"   Renamed 'index' to 'date'\")\n\n    if 'date' not in bulk_df.columns:\n        raise ValueError(f\"'date' column not found and could not be recovered. Columns: {bulk_df.columns.tolist()}\")\n\nprint(f\"Required columns verified: 'close' and 'date' present\")\n\n# 5. Filter quality stocks (parallel processing for speed)\nprint(\"\\n\" + \"=\"*60)\nprint(\"FILTERING QUALITY STOCKS\")\nprint(\"=\"*60)\nprint(f\"Before filter: {len(bulk_df):,} rows\")\n\nbulk_df = filter_quality_stocks(\n    bulk_df,\n    min_price=CONFIG['min_price'],\n    min_volume=CONFIG['min_volume']\n)\n\nprint(f\"After filter: {len(bulk_df):,} rows\")\nprint(f\"Removed: {total_rows - len(bulk_df):,} low-quality data points\")\nprint(f\"Remaining stocks: {bulk_df['symbol'].nunique()}\")\n\n# 6. Add momentum features (vectorized for speed)\nprint(\"\\n\" + \"=\"*60)\nprint(\"CALCULATING MOMENTUM FEATURES\")\nprint(\"=\"*60)\n\nstart_time = time.time()\nbulk_df = calculate_momentum_features(bulk_df)\ncalc_time = time.time() - start_time\n\nprint(f\"Calculated in {calc_time:.1f} seconds\")\nprint(f\"Features added: RSI, MACD, ROC, ATR, Volume Ratio\")\n\n# 7. Preprocess with trend filter and sustained breakout definition\nprint(\"\\n\" + \"=\"*60)\nprint(\"LABELING SUSTAINED BREAKOUTS\")\nprint(\"=\"*60)\n\nbulk_df = preprocess_data(bulk_df)\n\nprint(f\"\\nFINAL DATASET:\")\nprint(f\"   Total rows: {len(bulk_df):,}\")\nprint(f\"   Unique stocks: {bulk_df['symbol'].nunique()}\")\nprint(f\"   Date range: {bulk_df['date'].min()} to {bulk_df['date'].max()}\")\nprint(f\"   Breakouts found: {bulk_df['breakout'].sum():,}\")\nprint(f\"   Breakout rate: {bulk_df['breakout'].mean():.2%}\")\n\n# 8. Validate sufficient training data\nif bulk_df['breakout'].sum() < 50:\n    print(\"\\nWARNING: Very few breakouts found (<50). Consider:\")\n    print(\"   - Lowering return thresholds in preprocess_data()\")\n    print(\"   - Using more historical data (start_date='2015-01-01')\")\n    print(\"   - Checking data quality\")\nelif bulk_df['breakout'].sum() < 200:\n    print(\"\\nCAUTION: Limited breakouts found (<200). Model may be less robust.\")\nelse:\n    print(f\"\\nSufficient training data: {bulk_df['breakout'].sum()} breakouts\")\n\n# 9. Select features and prepare training data\nprint(\"\\n\" + \"=\"*60)\nprint(\"PREPARING TRAINING DATA\")\nprint(\"=\"*60)\n\nfeatures = [\n    'open', 'high', 'low', 'close', 'volume',\n    'sma_50', 'sma_200', 'rsi', 'macd', 'macd_histogram',\n    'roc_10', 'atr', 'volume_ratio'\n]\n\nX = bulk_df[features].fillna(0)\ny = bulk_df['breakout']\n\nprint(f\"Feature matrix shape: {X.shape}\")\nprint(f\"Features: {len(features)}\")\nprint(f\"Class distribution:\")\nprint(f\"   No Breakout: {(y==0).sum():,} ({(y==0).mean():.1%})\")\nprint(f\"   Breakout:    {(y==1).sum():,} ({(y==1).mean():.1%})\")\n\n# 10. Train/test split with stratification\nprint(\"\\n\" + \"=\"*60)\nprint(\"SPLITTING TRAIN/TEST DATA\")\nprint(\"=\"*60)\n\ntry:\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42, stratify=y\n    )\n    print(\"Stratified split successful\")\nexcept ValueError as e:\n    print(f\"Cannot stratify: {e}\")\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n\nprint(f\"Train: {len(X_train):,} samples ({y_train.sum():,} breakouts, {y_train.mean():.2%})\")\nprint(f\"Test:  {len(X_test):,} samples ({y_test.sum():,} breakouts, {y_test.mean():.2%})\")\n\n# 11. Train model with optimized hyperparameters\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING XGBOOST CLASSIFIER\")\nprint(\"=\"*60)\n\n# Calculate class weight to handle imbalance\npos_count = y_train.sum()\nneg_count = len(y_train) - pos_count\nscale_pos_weight = neg_count / pos_count if pos_count > 0 else 1\n\nprint(f\"Scale pos weight: {scale_pos_weight:.2f}\")\n\nclf = BreakoutStockClassifier()\nclf.model = XGBClassifier(\n    n_estimators=300,\n    max_depth=6,\n    learning_rate=0.05,\n    scale_pos_weight=scale_pos_weight,\n    random_state=42,\n    eval_metric='logloss',\n    n_jobs=-1,  # Use all CPU cores\n    tree_method='hist'  # Faster for large datasets\n)\n\ntrain_start = time.time()\nclf.fit(X_train, y_train)\ntrain_time = time.time() - train_start\n\nprint(f\"Training complete in {train_time:.1f} seconds!\")\n\n# 12. Evaluate model\nprint(\"\\n\" + \"=\"*60)\nprint(\"MODEL EVALUATION\")\nprint(\"=\"*60)\n\nevaluate_model(clf.model, X_test, y_test)\n\n# 13. Feature importance\nprint(\"\\n\" + \"=\"*60)\nprint(\"TOP 10 MOST IMPORTANT FEATURES\")\nprint(\"=\"*60)\n\nfeature_importance = pd.DataFrame({\n    'feature': features,\n    'importance': clf.model.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(feature_importance.head(10).to_string(index=False))\n\n# 14. Save model with metadata - detect environment\nprint(\"\\n\" + \"=\"*60)\nprint(\"SAVING MODEL\")\nprint(\"=\"*60)\n\n# Detect environment and set output directory\nif os.path.exists('/kaggle/working'):\n    # Kaggle environment - save to working directory (shows in Output tab)\n    model_dir = '/kaggle/working'\n    print(\"Detected Kaggle environment\")\nelif os.path.exists('/content'):\n    # Colab environment\n    model_dir = '/content/ml_models'\n    print(\"Detected Colab environment\")\nelse:\n    # Local environment\n    model_dir = '../python/ml_models'\n    print(\"Detected local environment\")\n\nos.makedirs(model_dir, exist_ok=True)\n\n# Save model with descriptive name\nmodel_filename = 'breakout_classifier_xgb_stockslist.pkl'\nmodel_path = f'{model_dir}/{model_filename}'\nclf.save(model_path)\n\n# Save metadata\nmetadata = {\n    'training_date': time.strftime('%Y-%m-%d %H:%M:%S'),\n    'total_stocks': len(all_tickers),\n    'successful_stocks': bulk_df['symbol'].nunique(),\n    'failed_stocks': len(failed_tickers),\n    'training_samples': len(X_train),\n    'test_samples': len(X_test),\n    'breakout_rate': float(y.mean()),\n    'features': features,\n    'config': CONFIG,\n    'model_performance': {\n        'training_time_seconds': train_time,\n        'test_breakouts': int(y_test.sum())\n    }\n}\n\nimport json\nmetadata_path = f'{model_dir}/breakout_classifier_metadata.json'\nwith open(metadata_path, 'w') as f:\n    json.dump(metadata, f, indent=2)\n\nprint(f\"Model saved to: {model_path}\")\nprint(f\"Metadata saved to: {metadata_path}\")\n\n# 15. Summary\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING COMPLETE!\")\nprint(\"=\"*60)\nprint(f\"Trained on {bulk_df['symbol'].nunique()} stocks\")\nprint(f\"Found {bulk_df['breakout'].sum():,} sustained breakouts\")\nprint(f\"Model finds SUSTAINED breakouts in UPTRENDING stocks\")\nprint(f\"Model location: {model_path}\")\n\nif '/kaggle/' in model_dir:\n    print(\"\\nTo download: Go to Output tab on the right\")\nelif '/content/' in model_dir:\n    print(\"\\nNext steps:\")\n    print(\"   1. Run the download cell below to get a zip file\")\n    print(\"   2. Upload to your python/ml_models/ directory\")\n    print(\"   3. Use in your ML API for predictions\")\nelse:\n    print(\"\\nNext steps:\")\n    print(\"   1. Model is saved locally\")\n    print(\"   2. Use in your ML API for predictions\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "id": "3e1c9557",
   "metadata": {},
   "source": [
    "## 5. Improved Breakout Detection\n",
    "\n",
    "This updated notebook now finds **real, sustained breakouts** instead of one-day spikes:\n",
    "\n",
    "### Key Improvements:\n",
    "\n",
    "1. **Trend Filter**: Only considers stocks in uptrends (price above 50 & 200-day MA)\n",
    "2. **Sustained Returns**: Requires positive returns at 30, 60, AND 90 days:\n",
    "   - 20%+ at 30 days\n",
    "   - 30%+ at 60 days  \n",
    "   - 40%+ at 90 days\n",
    "3. **Volume Confirmation**: Breakouts need above-average volume (1.2x+)\n",
    "4. **Quality Filter**: Excludes penny stocks (<$5) and illiquid stocks (<500K volume)\n",
    "5. **Drawdown Protection**: Excludes stocks that crash >15% mid-period\n",
    "6. **Momentum Features**: RSI, MACD, ROC help identify real momentum\n",
    "\n",
    "### Expected Results:\n",
    "- Fewer breakout labels (more selective)\n",
    "- Higher quality picks that sustain growth\n",
    "- Better for real trading/investment decisions\n",
    "\n",
    "### If you get too few breakouts:\n",
    "- Lower the return thresholds (e.g., 15%/25%/35%)\n",
    "- Increase the stock universe (use more tickers)\n",
    "- Use more historical data (start_date=\"2015-01-01\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lteabku7efk",
   "metadata": {},
   "source": [
    "## 6. Download Models (For Colab Users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kaozm4kg66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download models for Colab (UNCOMMENT if running in Colab)\n",
    "# from google.colab import files\n",
    "# import zipfile\n",
    "# import os\n",
    "\n",
    "# print(\"üì¶ Creating zip file with breakout classifier models...\")\n",
    "\n",
    "# # Create zip file\n",
    "# zip_filename = 'breakout_classifier_models.zip'\n",
    "# with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "#     for file in ['breakout_classifier_xgb_stockslist.pkl', 'breakout_classifier_metadata.json']:\n",
    "#         file_path = f\"{model_dir}/{file}\"\n",
    "#         if os.path.exists(file_path):\n",
    "#             zipf.write(file_path, file)\n",
    "#             print(f\"  ‚úÖ Added {file}\")\n",
    "#         else:\n",
    "#             print(f\"  ‚ö†Ô∏è  {file} not found\")\n",
    "\n",
    "# print(f\"\\n‚¨áÔ∏è  Downloading {zip_filename}...\")\n",
    "# files.download(zip_filename)\n",
    "# print(\"‚úÖ Download complete!\")\n",
    "# print(\"\\nüìã Next steps:\")\n",
    "# print(\"1. Extract the zip file\")\n",
    "# print(\"2. Upload the .pkl file to your project's python/ml_models/ directory\")\n",
    "# print(\"3. Use the model in your ML API for breakout predictions\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}