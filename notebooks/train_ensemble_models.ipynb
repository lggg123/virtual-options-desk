{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8cf9350",
   "metadata": {},
   "source": "# Train Ensemble Stock Screening Models\n\nThis notebook trains the Pro tier ensemble models:\n- XGBoost (35% weight)\n- Random Forest (25% weight)\n- LightGBM (40% weight)\n\n**Training Time:** 10-30 minutes  \n**Output:** xgboost.pkl, random_forest.pkl, lightgbm.pkl, feature_engineer.pkl, metadata.json  \n**Recommended:** Run in Google Colab for faster training\n\n---\n\n## ðŸš€ Quick Start Guide\n\n### For Google Colab Users:\n\n1. **Upload this notebook to Colab**\n   - Go to [colab.research.google.com](https://colab.research.google.com)\n   - File â†’ Upload notebook â†’ Select this file\n\n2. **Install packages** (Cell 2 below)\n   - Uncomment the install line\n   - Run the cell\n\n3. **Upload your stock list CSV** (Optional)\n   - Upload `stocks-list.csv` (curated ~5500 major exchange stocks):\n   ```python\n   from google.colab import files\n   uploaded = files.upload()\n   ```\n   - Or skip this - the notebook will download S&P 500 stocks automatically\n\n4. **Run all cells**\n   - Runtime â†’ Run all\n   - Go get coffee â˜• (takes 10-30 minutes)\n\n5. **Download trained models**\n   - Run the last cell to download a zip file with all models\n   - Upload these to your `ml_models/` directory\n\n### For Local Users:\n\n1. **Install dependencies:**\n   ```bash\n   pip install xgboost lightgbm scikit-learn pandas numpy yfinance joblib\n   ```\n\n2. **Run all cells in order**\n\n3. **Models save to:** `../ml_models/`\n\n---"
  },
  {
   "cell_type": "markdown",
   "id": "3ae9a8e3",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd3eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (UNCOMMENT if running in Colab)\n",
    "# !pip install xgboost lightgbm scikit-learn pandas numpy yfinance joblib -q\n",
    "\n",
    "# Optional: Upload your stock list CSV in Colab\n",
    "# from google.colab import files\n",
    "# print(\"ðŸ“ Upload your eodhd_us_tickers.csv or stocks-list.csv file:\")\n",
    "# uploaded = files.upload()\n",
    "# # Move to data directory\n",
    "# import os\n",
    "# os.makedirs('data', exist_ok=True)\n",
    "# for filename in uploaded.keys():\n",
    "#     os.rename(filename, f'data/{filename}')\n",
    "#     print(f\"âœ… Uploaded {filename} to data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129dce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f905311",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b072ea8",
   "metadata": {},
   "outputs": [],
   "source": "# Training configuration\nCONFIG = {\n    'training_universe_size': None,  # None = use ALL stocks from CSV (recommended)\n    'history_days': 730,             # 2 years of historical data\n    'forward_prediction_days': 30,   # Predict 30-day returns\n    'test_size': 0.2,                # 20% for testing\n    'random_state': 42,\n    'output_dir': 'ml_models',       # Where to save models\n}\n\n# Model weights for ensemble\nMODEL_WEIGHTS = {\n    'xgboost': 0.35,\n    'random_forest': 0.25,\n    'lightgbm': 0.40,\n}\n\nif CONFIG['training_universe_size']:\n    print(f\"Training on {CONFIG['training_universe_size']} stocks\")\nelse:\n    print(\"Training on ALL stocks from CSV (recommended for best results)\")\nprint(f\"Using {CONFIG['history_days']} days of historical data\")\nprint(f\"Predicting {CONFIG['forward_prediction_days']}-day returns\")"
  },
  {
   "cell_type": "markdown",
   "id": "5eb15ec4",
   "metadata": {},
   "source": [
    "## 3. Load Stock Universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e58cc7",
   "metadata": {},
   "outputs": [],
   "source": "# Load stock list from CSV or download S&P 500\nimport os\n\nuniverse = []  # Initialize to prevent NameError\n\ntry:\n    # Try multiple possible paths (Kaggle, Colab, Local)\n    # NOTE: Using stocks-list.csv (curated ~5500 stocks) instead of eodhd_us_tickers.csv\n    # to avoid OTC/PINK stocks that cause timeouts and have poor data availability\n    for path in [\n        # Kaggle paths (when using \"Add Data\" feature)\n        '/kaggle/input/stocks-list/stocks-list.csv',\n        '/kaggle/input/stock-list/stocks-list.csv',\n        '/kaggle/working/stocks-list.csv',\n        # Colab paths\n        '/content/stocks-list.csv',\n        # Current directory\n        'stocks-list.csv',\n        # Local development paths\n        '../data/stocks-list.csv',\n        'data/stocks-list.csv',\n    ]:\n        if os.path.exists(path):\n            stocks_df = pd.read_csv(path)\n            # Try different column names\n            for col in ['Symbol', 'symbol', 'ticker', 'Ticker', 'SYMBOL', 'Code']:\n                if col in stocks_df.columns:\n                    # Use all stocks if training_universe_size is None\n                    if CONFIG['training_universe_size'] is None:\n                        universe = stocks_df[col].dropna().tolist()\n                    else:\n                        universe = stocks_df[col].head(CONFIG['training_universe_size']).tolist()\n                    print(f\"âœ… Loaded {len(universe)} stocks from {path}\")\n                    break\n            if universe:  # Break outer loop if we found stocks\n                break\n    \n    if not universe:\n        raise FileNotFoundError(\"No valid CSV found\")\n        \nexcept Exception as e:\n    # Fallback: Download S&P 500 stocks\n    print(f\"âš ï¸  CSV not found ({e}), downloading S&P 500 list...\")\n    sp500_table = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n    if CONFIG['training_universe_size'] is None:\n        universe = sp500_table['Symbol'].tolist()\n    else:\n        universe = sp500_table['Symbol'].head(CONFIG['training_universe_size']).tolist()\n    print(f\"âœ… Loaded {len(universe)} stocks from S&P 500\")\n\nprint(f\"\\nTotal stocks: {len(universe)}\")\nprint(f\"First 10: {universe[:10]}\")\nprint(f\"\\nâ±ï¸  ESTIMATED TRAINING TIME:\")\nprint(f\"   - 500 stocks: ~10-30 minutes\")\nprint(f\"   - 2500 stocks: ~1-2 hours\")\nprint(f\"   - 5000+ stocks: ~2-4 hours\")\nprint(f\"\\nðŸ’¡ TIP: More stocks = more robust model!\")"
  },
  {
   "cell_type": "markdown",
   "id": "194456e6",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291a64ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_technical_features(df):\n",
    "    \"\"\"\n",
    "    Calculate technical indicators from OHLCV data\n",
    "    Returns 19 technical features\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Price-based features\n",
    "    features['close'] = df['Close'].iloc[-1]\n",
    "    features['volume'] = df['Volume'].iloc[-1]\n",
    "    \n",
    "    # Returns\n",
    "    features['return_1d'] = df['Close'].pct_change(1).iloc[-1]\n",
    "    features['return_5d'] = df['Close'].pct_change(5).iloc[-1]\n",
    "    features['return_20d'] = df['Close'].pct_change(20).iloc[-1]\n",
    "    features['return_60d'] = df['Close'].pct_change(60).iloc[-1]\n",
    "    \n",
    "    # Volatility\n",
    "    features['volatility_20d'] = df['Close'].pct_change().rolling(20).std().iloc[-1]\n",
    "    features['volatility_60d'] = df['Close'].pct_change().rolling(60).std().iloc[-1]\n",
    "    \n",
    "    # Moving averages\n",
    "    features['sma_20'] = df['Close'].rolling(20).mean().iloc[-1]\n",
    "    features['sma_50'] = df['Close'].rolling(50).mean().iloc[-1]\n",
    "    features['sma_200'] = df['Close'].rolling(200).mean().iloc[-1]\n",
    "    \n",
    "    # Moving average crossovers\n",
    "    features['sma_20_50_ratio'] = features['sma_20'] / features['sma_50'] if features['sma_50'] > 0 else 1\n",
    "    features['sma_50_200_ratio'] = features['sma_50'] / features['sma_200'] if features['sma_200'] > 0 else 1\n",
    "    \n",
    "    # RSI (14-day)\n",
    "    delta = df['Close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "    rs = gain / loss\n",
    "    features['rsi_14'] = 100 - (100 / (1 + rs.iloc[-1]))\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    bb_middle = df['Close'].rolling(20).mean()\n",
    "    bb_std = df['Close'].rolling(20).std()\n",
    "    bb_upper = bb_middle + (bb_std * 2)\n",
    "    bb_lower = bb_middle - (bb_std * 2)\n",
    "    features['bb_position'] = (df['Close'].iloc[-1] - bb_lower.iloc[-1]) / (bb_upper.iloc[-1] - bb_lower.iloc[-1]) if (bb_upper.iloc[-1] - bb_lower.iloc[-1]) > 0 else 0.5\n",
    "    \n",
    "    # Volume features\n",
    "    features['volume_20d_avg'] = df['Volume'].rolling(20).mean().iloc[-1]\n",
    "    features['volume_ratio'] = features['volume'] / features['volume_20d_avg'] if features['volume_20d_avg'] > 0 else 1\n",
    "    \n",
    "    # Price momentum\n",
    "    features['momentum_20d'] = df['Close'].iloc[-1] / df['Close'].iloc[-20] - 1 if len(df) >= 20 else 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "def calculate_forward_return(df, days=30):\n",
    "    \"\"\"\n",
    "    Calculate forward return (label for training)\n",
    "    \"\"\"\n",
    "    if len(df) < days:\n",
    "        return None\n",
    "    return (df['Close'].iloc[-1] / df['Close'].iloc[-days] - 1)\n",
    "\n",
    "print(\"âœ… Feature engineering functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c2cd9a",
   "metadata": {},
   "source": [
    "## 5. Download Historical Data & Build Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7059c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“Š Downloading historical data...\\n\")\n",
    "\n",
    "all_features = []\n",
    "all_labels = []\n",
    "failed_tickers = []\n",
    "\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=CONFIG['history_days'])\n",
    "\n",
    "for i, symbol in enumerate(universe):\n",
    "    try:\n",
    "        # Download data\n",
    "        ticker = yf.Ticker(symbol)\n",
    "        df = ticker.history(start=start_date, end=end_date)\n",
    "        \n",
    "        if len(df) < 200:  # Need at least 200 days for features\n",
    "            failed_tickers.append(symbol)\n",
    "            continue\n",
    "        \n",
    "        # Generate samples by sliding window\n",
    "        for j in range(200, len(df) - CONFIG['forward_prediction_days']):\n",
    "            window = df.iloc[:j+1].copy()\n",
    "            future_window = df.iloc[j:j+CONFIG['forward_prediction_days']+1].copy()\n",
    "            \n",
    "            # Calculate features\n",
    "            features = calculate_technical_features(window)\n",
    "            features['symbol'] = symbol\n",
    "            \n",
    "            # Calculate forward return (label)\n",
    "            label = calculate_forward_return(future_window, CONFIG['forward_prediction_days'])\n",
    "            \n",
    "            if label is not None and not np.isnan(label) and all(not np.isnan(v) and not np.isinf(v) for v in features.values() if isinstance(v, float)):\n",
    "                all_features.append(features)\n",
    "                all_labels.append(label)\n",
    "        \n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"Processed {i+1}/{len(universe)} stocks, Generated {len(all_features)} samples\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        failed_tickers.append(symbol)\n",
    "        continue\n",
    "\n",
    "print(f\"\\nâœ… Data collection complete!\")\n",
    "print(f\"Total samples: {len(all_features)}\")\n",
    "print(f\"Failed tickers: {len(failed_tickers)}\")\n",
    "if failed_tickers:\n",
    "    print(f\"Failed: {failed_tickers[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c72bd4",
   "metadata": {},
   "source": [
    "## 6. Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d53ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "features_df = pd.DataFrame(all_features)\n",
    "labels_series = pd.Series(all_labels, name='forward_return')\n",
    "\n",
    "print(f\"Features shape: {features_df.shape}\")\n",
    "print(f\"Labels shape: {labels_series.shape}\")\n",
    "\n",
    "# Separate symbol column\n",
    "symbols = features_df['symbol']\n",
    "features_df = features_df.drop('symbol', axis=1)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features_df, labels_series, \n",
    "    test_size=CONFIG['test_size'], \n",
    "    random_state=CONFIG['random_state']\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining samples: {len(X_train)}\")\n",
    "print(f\"Testing samples: {len(X_test)}\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nâœ… Data prepared and scaled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4bbb20",
   "metadata": {},
   "source": [
    "## 7. Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592a23eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "models = {}\n",
    "results = {}\n",
    "\n",
    "print(\"ðŸš€ Training models...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3416f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. XGBoost (35% weight)\n",
    "print(\"Training XGBoost...\")\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=CONFIG['random_state'],\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "results['xgboost'] = {\n",
    "    'mse': mean_squared_error(y_test, xgb_pred),\n",
    "    'mae': mean_absolute_error(y_test, xgb_pred),\n",
    "    'r2': r2_score(y_test, xgb_pred)\n",
    "}\n",
    "models['xgboost'] = xgb_model\n",
    "\n",
    "print(f\"âœ… XGBoost trained - RÂ²: {results['xgboost']['r2']:.4f}, MAE: {results['xgboost']['mae']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18ed364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Random Forest (25% weight)\n",
    "print(\"\\nTraining Random Forest...\")\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=CONFIG['random_state'],\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "rf_pred = rf_model.predict(X_test_scaled)\n",
    "\n",
    "results['random_forest'] = {\n",
    "    'mse': mean_squared_error(y_test, rf_pred),\n",
    "    'mae': mean_absolute_error(y_test, rf_pred),\n",
    "    'r2': r2_score(y_test, rf_pred)\n",
    "}\n",
    "models['random_forest'] = rf_model\n",
    "\n",
    "print(f\"âœ… Random Forest trained - RÂ²: {results['random_forest']['r2']:.4f}, MAE: {results['random_forest']['mae']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffbaecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. LightGBM (40% weight)\n",
    "print(\"\\nTraining LightGBM...\")\n",
    "lgbm_model = LGBMRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=CONFIG['random_state'],\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "lgbm_model.fit(X_train_scaled, y_train)\n",
    "lgbm_pred = lgbm_model.predict(X_test_scaled)\n",
    "\n",
    "results['lightgbm'] = {\n",
    "    'mse': mean_squared_error(y_test, lgbm_pred),\n",
    "    'mae': mean_absolute_error(y_test, lgbm_pred),\n",
    "    'r2': r2_score(y_test, lgbm_pred)\n",
    "}\n",
    "models['lightgbm'] = lgbm_model\n",
    "\n",
    "print(f\"âœ… LightGBM trained - RÂ²: {results['lightgbm']['r2']:.4f}, MAE: {results['lightgbm']['mae']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46373d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Ensemble predictions\n",
    "print(\"\\nCalculating ensemble predictions...\")\n",
    "ensemble_pred = (\n",
    "    MODEL_WEIGHTS['xgboost'] * xgb_pred +\n",
    "    MODEL_WEIGHTS['random_forest'] * rf_pred +\n",
    "    MODEL_WEIGHTS['lightgbm'] * lgbm_pred\n",
    ")\n",
    "\n",
    "results['ensemble'] = {\n",
    "    'mse': mean_squared_error(y_test, ensemble_pred),\n",
    "    'mae': mean_absolute_error(y_test, ensemble_pred),\n",
    "    'r2': r2_score(y_test, ensemble_pred)\n",
    "}\n",
    "\n",
    "print(f\"âœ… Ensemble - RÂ²: {results['ensemble']['r2']:.4f}, MAE: {results['ensemble']['mae']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8344d6e8",
   "metadata": {},
   "source": [
    "## 8. Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bc8eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“Š MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df[['r2', 'mae', 'mse']]\n",
    "results_df.columns = ['RÂ² Score', 'MAE', 'MSE']\n",
    "print(results_df.to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Best Model: {results_df['RÂ² Score'].idxmax()} (RÂ² = {results_df['RÂ² Score'].max():.4f})\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99437015",
   "metadata": {},
   "source": [
    "## 9. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491ec8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from XGBoost\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': features_df.columns,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebf362e",
   "metadata": {},
   "source": [
    "## 10. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47647c31",
   "metadata": {},
   "outputs": [],
   "source": "import os\n\n# Detect environment and set output directory\nif os.path.exists('/kaggle/working'):\n    # Kaggle environment - save to working directory (shows in Output tab)\n    output_dir = '/kaggle/working'\n    print(\"ðŸ“ Detected Kaggle environment\")\nelif os.path.exists('/content'):\n    # Colab environment\n    output_dir = '/content/ml_models'\n    print(\"ðŸ“ Detected Colab environment\")\nelse:\n    # Local environment\n    output_dir = f\"../{CONFIG['output_dir']}\"\n    print(\"ðŸ“ Detected local environment\")\n\nos.makedirs(output_dir, exist_ok=True)\nprint(f\"ðŸ’¾ Saving models to {output_dir}/...\\n\")\n\n# Save models\njoblib.dump(models['xgboost'], f\"{output_dir}/xgboost.pkl\")\nprint(\"âœ… Saved xgboost.pkl\")\n\njoblib.dump(models['random_forest'], f\"{output_dir}/random_forest.pkl\")\nprint(\"âœ… Saved random_forest.pkl\")\n\njoblib.dump(models['lightgbm'], f\"{output_dir}/lightgbm.pkl\")\nprint(\"âœ… Saved lightgbm.pkl\")\n\njoblib.dump(scaler, f\"{output_dir}/feature_engineer.pkl\")\nprint(\"âœ… Saved feature_engineer.pkl (scaler)\")\n\n# Save metadata\nmetadata = {\n    'training_date': datetime.now().isoformat(),\n    'num_stocks': len(universe) - len(failed_tickers),\n    'training_samples': len(X_train),\n    'test_samples': len(X_test),\n    'feature_count': len(features_df.columns),\n    'features': list(features_df.columns),\n    'model_weights': MODEL_WEIGHTS,\n    'performance': results,\n    'config': CONFIG\n}\n\nwith open(f\"{output_dir}/metadata.json\", 'w') as f:\n    json.dump(metadata, f, indent=2)\nprint(\"âœ… Saved metadata.json\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"ðŸŽ‰ TRAINING COMPLETE!\")\nprint(\"=\"*60)\nprint(f\"\\nModels saved to: {output_dir}/\")\nprint(\"\\nFiles created:\")\nprint(\"  - xgboost.pkl\")\nprint(\"  - random_forest.pkl\")\nprint(\"  - lightgbm.pkl\")\nprint(\"  - feature_engineer.pkl\")\nprint(\"  - metadata.json\")\n\nif '/kaggle/' in output_dir:\n    print(\"\\nðŸ“¥ To download: Go to Output tab on the right â†’\")\nelif '/content/' in output_dir:\n    print(\"\\nðŸ“¥ Run the download cell below to get your models\")\nelse:\n    print(\"\\nâœ… Models ready in ml_models/ directory\")"
  },
  {
   "cell_type": "markdown",
   "id": "14fa49e8",
   "metadata": {},
   "source": [
    "## 11. Download Models (For Colab Users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af009b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download models for Colab (UNCOMMENT if running in Colab)\n",
    "# from google.colab import files\n",
    "# import zipfile\n",
    "# import os\n",
    "\n",
    "# print(\"ðŸ“¦ Creating zip file with all models...\")\n",
    "\n",
    "# # Create zip file\n",
    "# zip_filename = 'ml_models.zip'\n",
    "# with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "#     for file in ['xgboost.pkl', 'random_forest.pkl', 'lightgbm.pkl', 'feature_engineer.pkl', 'metadata.json']:\n",
    "#         file_path = f\"{output_dir}/{file}\"\n",
    "#         if os.path.exists(file_path):\n",
    "#             zipf.write(file_path, file)\n",
    "#             print(f\"  âœ… Added {file}\")\n",
    "#         else:\n",
    "#             print(f\"  âš ï¸  {file} not found\")\n",
    "\n",
    "# print(f\"\\nâ¬‡ï¸  Downloading {zip_filename}...\")\n",
    "# files.download(zip_filename)\n",
    "# print(\"âœ… Download complete!\")\n",
    "# print(\"\\nðŸ“‹ Next steps:\")\n",
    "# print(\"1. Extract the zip file\")\n",
    "# print(\"2. Upload the .pkl files to your project's ml_models/ directory\")\n",
    "# print(\"3. Deploy your ML API service with these models\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}